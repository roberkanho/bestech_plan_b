

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Working with the Entity Recognizer &mdash; The Conversational AI Playbook 4.0.2 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'../',
              VERSION:'4.0.2',
              LANGUAGE:'None',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/custom.js"></script>
        <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@1/dist/clipboard.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Using LSTM for Entity Recognition" href="lstm.html" />
    <link rel="prev" title="Working with the Intent Classifier" href="intent_classifier.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> The Conversational AI Playbook
          

          
          </a>

          
            
            
              <div class="version">
                4.0.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/introduction_to_conversational_applications.html">Introduction to Conversational Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/approaches_for_building_conversational_applications.html">Different Approaches for Building Conversational Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/anatomy_of_a_conversational_ai_interaction.html">Anatomy of a Conversational AI Interaction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/introducing_mindmeld_workbench.html">Introducing MindMeld</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/key_concepts.html">Key Concepts</a></li>
</ul>
<p class="caption"><span class="caption-text">Step-by-Step Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/00_overview.html">Building a Conversational Interface in 10 Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/01_select_the_right_use_case.html">Step 1: Select the Right Use Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/02_script_interactions.html">Step 2: Script Your Ideal Dialogue Interactions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/03_define_the_hierarchy.html">Step 3: Define the Domain, Intent, Entity, and Role Hierarchy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/04_define_the_dialogue_handlers.html">Step 4: Define the Dialogue State Handlers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/05_create_the_knowledge_base.html">Step 5: Create the Knowledge Base</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/06_generate_representative_training_data.html">Step 6: Generate Representative Training Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/07_train_the_natural_language_processing_classifiers.html">Step 7: Train the Natural Language Processing Classifiers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/08_configure_the_language_parser.html">Step 8: Configure the Language Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/09_optimize_question_answering_performance.html">Step 9: Optimize Question Answering Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/10_deploy_to_production.html">Step 10: Deploy Trained Models</a></li>
</ul>
<p class="caption"><span class="caption-text">Blueprint Applications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../blueprints/overview.html">MindMeld Blueprints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blueprints/food_ordering.html">Food Ordering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blueprints/video_discovery.html">Video Discovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blueprints/home_assistant.html">Home Assistant</a></li>
</ul>
<p class="caption"><span class="caption-text">Integrations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../integrations/webex_teams.html">Webex Teams Integration</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="about.html">About this guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Platform Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessor.html">Working with the Preprocessor</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Working with the Natural Language Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain_classifier.html">Working with the Domain Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="intent_classifier.html">Working with the Intent Classifier</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Working with the Entity Recognizer</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#system-entities-and-custom-entities">System entities and custom entities</a></li>
<li class="toctree-l2"><a class="reference internal" href="#access-the-entity-recognizer">Access the entity recognizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#train-an-entity-recognizer">Train an entity recognizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#classifier-configuration">Classifier configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-with-custom-configurations">Training with custom configurations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#application-configuration-file">1. Application configuration file</a></li>
<li class="toctree-l4"><a class="reference internal" href="#arguments-to-the-fit-method">2. Arguments to the <code class="docutils literal"><span class="pre">fit()</span></code> method</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#run-the-entity-recognizer">Run the entity recognizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#evaluate-classifier-performance">Evaluate classifier performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#viewing-features-extracted-for-entity-recognition">Viewing features extracted for entity recognition</a></li>
<li class="toctree-l2"><a class="reference internal" href="#save-model-for-future-use">Save model for future use</a></li>
<li class="toctree-l2"><a class="reference internal" href="#more-about-system-entities">More about system entities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#custom-entities-system-entities-and-training-set-size">Custom entities, system entities, and training set size</a></li>
<li class="toctree-l3"><a class="reference internal" href="#annotating-system-entities">Annotating system entities</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inspecting-how-workbench-detects-system-entities">Inspecting how Workbench detects system entities</a></li>
<li class="toctree-l3"><a class="reference internal" href="#when-workbench-is-unable-to-resolve-a-system-entity">When Workbench is unable to resolve a system entity</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lstm.html">Using LSTM for Entity Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="role_classifier.html">Working with the Role Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="entity_resolver.html">Working with the Entity Resolver</a></li>
<li class="toctree-l1"><a class="reference internal" href="parser.html">Working with the Language Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_features.html">Working with User-Defined Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="kb.html">Working with the Knowledge Base and Question Answerer</a></li>
<li class="toctree-l1"><a class="reference internal" href="dm.html">Working with the Dialogue Manager</a></li>
<li class="toctree-l1"><a class="reference internal" href="voice.html">Dealing with Voice Inputs</a></li>
</ul>
<p class="caption"><span class="caption-text">Versions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../versions/changes.html">Recent Changes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../versions/history.html">Package History</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/api_reference.html">API Reference</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">The Conversational AI Playbook</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Working with the Entity Recognizer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/userguide/entity_recognizer.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="working-with-the-entity-recognizer">
<h1>Working with the Entity Recognizer<a class="headerlink" href="#working-with-the-entity-recognizer" title="Permalink to this headline">¶</a></h1>
<p>The <a class="reference internal" href="architecture.html#arch-entity-model"><span class="std std-ref">Entity Recognizer</span></a></p>
<blockquote>
<div><ul class="simple">
<li>is run as the third step in the <a class="reference internal" href="nlp.html#instantiate-nlp"><span class="std std-ref">natural language processing pipeline</span></a></li>
<li>is a <a class="reference external" href="https://en.wikipedia.org/wiki/Sequence_labeling">sequence labeling</a> or tagging model that detects all the relevant <a class="reference internal" href="../intro/key_concepts.html#term-entity"><span class="xref std std-term">entities</span></a> in a given query</li>
<li>is trained per intent, using all the labeled queries for a given intent, with labels derived from the entity types annotated within the training queries</li>
</ul>
</div></blockquote>
<p>Every Workbench app has one entity recognizer for every intent that requires entity detection.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li>This is an in-depth tutorial to work through from start to finish. Before you begin, read the <a class="reference internal" href="../index.html#quickstart"><span class="std std-ref">Step-by-Step Guide</span></a>, paying special attention to the <a class="reference internal" href="../quickstart/07_train_the_natural_language_processing_classifiers.html#entity-recognition"><span class="std std-ref">Entity Recognition</span></a> section.</li>
<li>This section requires the <a class="reference internal" href="../blueprints/home_assistant.html"><span class="doc">Home Assistant</span></a> blueprint application. To get the app, open a terminal and run <code class="docutils literal"><span class="pre">mindmeld</span> <span class="pre">blueprint</span> <span class="pre">home_assistant</span></code>.</li>
</ul>
</div>
<div class="section" id="system-entities-and-custom-entities">
<h2>System entities and custom entities<a class="headerlink" href="#system-entities-and-custom-entities" title="Permalink to this headline">¶</a></h2>
<p>Entities in Workbench are categorized into two types:</p>
<dl class="docutils">
<dt><strong>System Entities</strong></dt>
<dd>Generic entities that are application-agnostic and are automatically detected by Workbench. Examples include numbers, time expressions, email addresses, URLs and measured quantities like distance, volume, currency and temperature. See <a class="reference internal" href="#system-entities"><span class="std std-ref">More about system entities</span></a> below.</dd>
<dt><strong>Custom Entities</strong></dt>
<dd>Application-specific entities that can only be detected by an entity recognizer that uses statistical models trained with deep domain knowledge. These are generally <a class="reference external" href="https://en.wikipedia.org/wiki/Named_entity">named entities</a>, like ‘San Bernardino,’ a proper name that could be a <code class="docutils literal"><span class="pre">location</span></code> entity. Custom entities that are <em>not</em> based on proper nouns (and therefore are not named entities) are also possible.</dd>
</dl>
<p>This chapter focuses on training entity recognition models for detecting all the custom entities used by your app.</p>
</div>
<div class="section" id="access-the-entity-recognizer">
<h2>Access the entity recognizer<a class="headerlink" href="#access-the-entity-recognizer" title="Permalink to this headline">¶</a></h2>
<p>Working with any natural language processor component falls into two broad phases:</p>
<blockquote>
<div><ul class="simple">
<li>First, generate the training data for your app. App performance largely depends on having sufficient quantity and quality of training data. See <a class="reference internal" href="../quickstart/06_generate_representative_training_data.html"><span class="doc">Step 6</span></a>.</li>
<li>Then, conduct experimentation in the Python shell.</li>
</ul>
</div></blockquote>
<p>When you are ready to begin experimenting, import the <code class="xref py py-class docutils literal"><span class="pre">NaturalLanguageProcessor</span></code> class from the Workbench <code class="xref py py-mod docutils literal"><span class="pre">nlp</span></code> module and instantiate an object with the path to your Workbench project.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindmeld.components.nlp</span> <span class="kn">import</span> <span class="n">NaturalLanguageProcessor</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">NaturalLanguageProcessor</span><span class="p">(</span><span class="n">app_path</span><span class="o">=</span><span class="s1">&#39;home_assistant&#39;</span><span class="p">)</span>
<span class="n">nlp</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">&lt;NaturalLanguageProcessor &#39;home_assistant&#39; ready: False, dirty: False&gt;</span>
</pre></div>
</div>
<p>Verify that the NLP has correctly identified all the domains and intents for your app.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">nlp</span><span class="o">.</span><span class="n">domains</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go"> &#39;greeting&#39;: &lt;DomainProcessor &#39;greeting&#39; ready: False, dirty: False&gt;,</span>
<span class="go"> &#39;smart_home&#39;: &lt;DomainProcessor &#39;smart_home&#39; ready: False, dirty: False&gt;,</span>
<span class="go"> &#39;times_and_dates&#39;: &lt;DomainProcessor &#39;times_and_dates&#39; ready: False, dirty: False&gt;,</span>
<span class="go"> &#39;unknown&#39;: &lt;DomainProcessor &#39;unknown&#39; ready: False, dirty: False&gt;,</span>
<span class="go"> &#39;weather&#39;: &lt;DomainProcessor &#39;weather&#39; ready: False, dirty: False&gt;</span>
<span class="go">}</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">nlp</span><span class="o">.</span><span class="n">domains</span><span class="p">[</span><span class="s1">&#39;times_and_dates&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">intents</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go"> &#39;change_alarm&#39;: &lt;IntentProcessor &#39;change_alarm&#39; ready: True, dirty: True&gt;,</span>
<span class="go"> &#39;check_alarm&#39;: &lt;IntentProcessor &#39;check_alarm&#39; ready: False, dirty: False&gt;,</span>
<span class="go"> &#39;remove_alarm&#39;: &lt;IntentProcessor &#39;remove_alarm&#39; ready: False, dirty: False&gt;,</span>
<span class="go"> &#39;set_alarm&#39;: &lt;IntentProcessor &#39;set_alarm&#39; ready: True, dirty: True&gt;,</span>
<span class="go"> &#39;start_timer&#39;: &lt;IntentProcessor &#39;start_timer&#39; ready: True, dirty: True&gt;,</span>
<span class="go"> &#39;stop_timer&#39;: &lt;IntentProcessor &#39;stop_timer&#39; ready: False, dirty: False&gt;</span>
<span class="go">}</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">nlp</span><span class="o">.</span><span class="n">domains</span><span class="p">[</span><span class="s1">&#39;weather&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">intents</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go"> &#39;check_weather&#39;: &lt;IntentProcessor &#39;check_weather&#39; ready: False, dirty: False&gt;</span>
<span class="go">}</span>
</pre></div>
</div>
<p>Access the <code class="xref py py-class docutils literal"><span class="pre">EntityRecognizer</span></code> for an intent of your choice, using the <code class="xref py py-attr docutils literal"><span class="pre">entity_recognizer</span></code> attribute of the desired intent.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Entity recognizer for the &#39;change_alarm&#39; intent in the &#39;times_and_dates&#39; domain:</span>
<span class="n">er</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">domains</span><span class="p">[</span><span class="s1">&#39;times_and_dates&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">intents</span><span class="p">[</span><span class="s1">&#39;change_alarm&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">entity_recognizer</span>
<span class="n">er</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">&lt;EntityRecognizer ready: False, dirty: False&gt;</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Entity recognizer for the &#39;check_weather&#39; intent in the &#39;weather&#39; domain:</span>
<span class="n">er</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">domains</span><span class="p">[</span><span class="s1">&#39;weather&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">intents</span><span class="p">[</span><span class="s1">&#39;check_weather&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">entity_recognizer</span>
<span class="n">er</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">&lt;EntityRecognizer ready: False, dirty: False&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="train-an-entity-recognizer">
<span id="train-entity-model"></span><h2>Train an entity recognizer<a class="headerlink" href="#train-an-entity-recognizer" title="Permalink to this headline">¶</a></h2>
<p>Use the <code class="xref py py-meth docutils literal"><span class="pre">EntityRecognizer.fit()</span></code> method to train an entity recognition model. Depending on the size of the training data and the selected model, this can take anywhere from a few seconds to several minutes. With logging level set to <code class="docutils literal"><span class="pre">INFO</span></code> or below, you should see the build progress in the console along with cross-validation accuracy of the trained model.</p>
<div class="highlight-python" id="baseline-entity-fit"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindmeld</span> <span class="kn">import</span> <span class="n">configure_logs</span><span class="p">;</span> <span class="n">configure_logs</span><span class="p">()</span>
<span class="n">er</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">domains</span><span class="p">[</span><span class="s1">&#39;weather&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">intents</span><span class="p">[</span><span class="s1">&#39;check_weather&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">entity_recognizer</span>
<span class="n">er</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">Fitting entity recognizer: domain=&#39;weather&#39;, intent=&#39;check_weather&#39;</span>
<span class="go">Loading raw queries from file home_assistant/domains/weather/check_weather/train.txt</span>
<span class="go">Loading queries from file home_assistant/domains/weather/check_weather/train.txt</span>
<span class="go">Selecting hyperparameters using k-fold cross validation with 5 splits</span>
<span class="go">Best accuracy: 99.14%, params: {&#39;C&#39;: 10000, &#39;penalty&#39;: &#39;l2&#39;}</span>
</pre></div>
</div>
<p>The <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method loads all necessary training queries and trains an entity recognition model. When called with no arguments (as in the example above), the method uses the settings from <code class="docutils literal"><span class="pre">config.py</span></code>, the <a class="reference internal" href="nlp.html#build-nlp-with-config"><span class="std std-ref">app’s configuration file</span></a>. If <code class="docutils literal"><span class="pre">config.py</span></code> is not defined, the method uses the Workbench preset <a class="reference internal" href="nlp.html#config"><span class="std std-ref">classifier configuration</span></a>.</p>
<p>Using default settings is the recommended (and quickest) way to get started with any of the NLP classifiers. The resulting baseline classifier should provide a reasonable starting point from which to bootstrap your machine learning experimentation. You can then try alternate settings as you seek to identify the optimal classifier configuration for your app.</p>
<div class="section" id="classifier-configuration">
<h3>Classifier configuration<a class="headerlink" href="#classifier-configuration" title="Permalink to this headline">¶</a></h3>
<p>Use the <code class="xref py py-attr docutils literal"><span class="pre">config</span></code> attribute of a trained classifier to view the <a class="reference internal" href="nlp.html#config"><span class="std std-ref">configuration</span></a> that the classifier is using. Here’s an example where we view the configuration of an entity recognizer trained using default settings:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">er</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">  &#39;features&#39;: {</span>
<span class="go">    &#39;bag-of-words-seq&#39;: {</span>
<span class="go">      &#39;ngram_lengths_to_start_positions&#39;: {</span>
<span class="go">         1: [-2, -1, 0, 1, 2],</span>
<span class="go">         2: [-2, -1, 0, 1]</span>
<span class="go">      }</span>
<span class="go">    },</span>
<span class="go">    &#39;in-gaz-span-seq&#39;: {},</span>
<span class="go">    &#39;sys-candidates-seq&#39;: {</span>
<span class="go">      &#39;start_positions&#39;: [-1, 0, 1]</span>
<span class="go">    }</span>
<span class="go">  },</span>
<span class="go">  &#39;model_settings&#39;: {</span>
<span class="go">    &#39;classifier_type&#39;: &#39;memm&#39;,</span>
<span class="go">    &#39;feature_scaler&#39;: &#39;max-abs&#39;,</span>
<span class="go">    &#39;tag_scheme&#39;: &#39;IOB&#39;</span>
<span class="go">  },</span>
<span class="go">  &#39;model_type&#39;: &#39;tagger&#39;,</span>
<span class="go">  &#39;param_selection&#39;: {</span>
<span class="go">    &#39;grid&#39;: {</span>
<span class="go">      &#39;C&#39;: [0.01, 1, 100, 10000, 1000000, 100000000],</span>
<span class="go">      &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;]</span>
<span class="go">    },</span>
<span class="go">   &#39;k&#39;: 5,</span>
<span class="go">   &#39;scoring&#39;: &#39;accuracy&#39;,</span>
<span class="go">   &#39;type&#39;: &#39;k-fold&#39;</span>
<span class="go">  },</span>
<span class="go">  &#39;params&#39;: None,</span>
<span class="go">  &#39;train_label_set&#39;: &#39;train.*\.txt&#39;,</span>
<span class="go">  &#39;test_label_set&#39;: &#39;test.*\.txt&#39;</span>
<span class="go">}</span>
</pre></div>
</div>
<p>Let’s take a look at the allowed values for each setting in an entity recognizer configuration.</p>
<ol class="arabic simple">
<li><strong>Model Settings</strong></li>
</ol>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">'model_type'</span></code> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><code class="xref py py-class docutils literal"><span class="pre">str</span></code></a>)</dt>
<dd><div class="first line-block">
<div class="line"><br /></div>
</div>
<p class="last">Always <code class="docutils literal"><span class="pre">'tagger'</span></code>, since the entity recognizer is a tagger model. <a class="reference external" href="https://en.wikipedia.org/wiki/Sequence_labeling">Tagging, sequence tagging, or sequence labeling</a> are common terms used in NLP literature for models that generate a tag for each token in a sequence. Taggers are most commonly used for part-of-speech tagging or named entity recognition.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">'model_settings'</span></code> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><code class="xref py py-class docutils literal"><span class="pre">dict</span></code></a>)</dt>
<dd><div class="first line-block">
<div class="line"><br /></div>
</div>
<p>A dictionary containing model-specific machine learning settings. The key <code class="docutils literal"><span class="pre">'classifier_type'</span></code>, whose value specifies the machine learning model to use, is required. Allowed values are shown in the table below.</p>
<table border="1" class="docutils" id="er-models">
<colgroup>
<col width="7%" />
<col width="45%" />
<col width="47%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Value</th>
<th class="head">Classifier</th>
<th class="head">Reference for configurable hyperparameters</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">'memm'</span></code></td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/Maximum-entropy_Markov_model">Maximum Entropy Markov Model</a></td>
<td><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">sklearn.linear_model.LogisticRegression</a></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">'crf'</span></code></td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/Conditional_random_field">Conditional Random Field</a></td>
<td><a class="reference external" href="https://sklearn-crfsuite.readthedocs.io/en/latest/api.html">sklearn-crfsuite</a></td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">'lstm'</span></code></td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/Long_short-term_memory">Long Short-Term Memory</a></td>
<td><a class="reference internal" href="lstm.html"><span class="doc">lstm API</span></a></td>
</tr>
</tbody>
</table>
<p>Tagger models allow you to specify the additional model settings shown below.</p>
<table border="1" class="last docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Key</th>
<th class="head">Value</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">'feature_scaler'</span></code></td>
<td><p class="first">The <a class="reference external" href="http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling">methodology</a> for
scaling raw feature values. Applicable to the MEMM model only.</p>
<p>Allowed values are:</p>
<ul class="last simple">
<li><code class="docutils literal"><span class="pre">'none'</span></code>: No scaling, i.e., use raw feature values.</li>
<li><code class="docutils literal"><span class="pre">'std-dev'</span></code>: Standardize features by removing the mean and scaling to unit variance. See
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler">StandardScaler</a>.</li>
<li><code class="docutils literal"><span class="pre">'max-abs'</span></code>: Scale each feature by its maximum absolute value. See
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler">MaxAbsScaler</a>.</li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">'tag_scheme'</span></code></td>
<td><p class="first">The tagging scheme for generating per-token labels.</p>
<p>Allowed values are:</p>
<ul class="last simple">
<li><code class="docutils literal"><span class="pre">'IOB'</span></code>: The <a class="reference external" href="https://en.wikipedia.org/wiki/Inside_Outside_Beginning">Inside-Outside-Beginning</a> tagging
format.</li>
<li><code class="docutils literal"><span class="pre">'IOBES'</span></code>: An extension to IOB where <code class="docutils literal"><span class="pre">'E'</span></code> represents the ending token in an entity span,
and <code class="docutils literal"><span class="pre">'S'</span></code> represents a single-token entity.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<ol class="arabic simple" start="2">
<li><strong>Feature Extraction Settings</strong></li>
</ol>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">'features'</span></code> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><code class="xref py py-class docutils literal"><span class="pre">dict</span></code></a>)</dt>
<dd><div class="first line-block">
<div class="line"><br /></div>
</div>
<p>A dictionary whose keys are names of feature groups to extract. The corresponding values are dictionaries representing the feature extraction settings for each group. The table below enumerates the features that can be used for entity recognition.</p>
<table border="1" class="last docutils" id="entity-features">
<colgroup>
<col width="20%" />
<col width="80%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Group Name</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">'bag-of-words-seq'</span></code></td>
<td><p class="first">Generates n-grams of specified lengths from the query text
surrounding the current token.</p>
<p>Settings:</p>
<p>A dictionary with n-gram lengths as keys
and a list of starting positions as values.
Each starting position is a token index,
relative to the current token.</p>
<p>Examples:</p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">'ngram_lengths_to_start_positions':</span> <span class="pre">{1:</span> <span class="pre">[0],</span> <span class="pre">2:</span> <span class="pre">[0]}</span></code></dt>
<dd><ul class="first last simple">
<li>extracts all words (unigrams) and bigrams starting with the current token</li>
</ul>
</dd>
<dt><code class="docutils literal"><span class="pre">'ngram_lengths_to_start_positions':</span> <span class="pre">{1:</span> <span class="pre">[-1,</span> <span class="pre">0,</span> <span class="pre">1],</span> <span class="pre">2:</span> <span class="pre">[-1,</span> <span class="pre">0,</span> <span class="pre">1]}</span></code></dt>
<dd><ul class="first last simple">
<li>additionally includes unigrams and bigrams starting from the words before and after the current token</li>
</ul>
</dd>
</dl>
<p>Given the query “weather in {San Francisco|location} {next week|sys_time}”
and a classifier extracting features for the token “Francisco”:</p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">{1:</span> <span class="pre">[-1,</span> <span class="pre">0,</span> <span class="pre">1]}</span></code></dt>
<dd><ul class="first last simple">
<li>extracts “San”, “Francisco”, and “next”</li>
</ul>
</dd>
<dt><code class="docutils literal"><span class="pre">{2:</span> <span class="pre">[-1,</span> <span class="pre">0,</span> <span class="pre">1]}</span></code></dt>
<dd><ul class="first last simple">
<li>extracts “in San”, “San Francisco”, and “Francisco next”</li>
</ul>
</dd>
</dl>
<p>Additionally, you can also limit the n-grams considered while extracting the feature by setting a
threshold on their frequency. These frequencies are computed over the entire training set. This prevents
infrequent n-grams from being used as features. By default, the threshold is set to 0.</p>
<p>Example:</p>
<blockquote class="last">
<div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s1">&#39;ngram_lengths_to_start_positions&#39;</span><span class="p">:</span> <span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
  <span class="s1">&#39;thresholds&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li>extracts all bigrams starting with current token and previous token whose frequency in the training
set is 5 or greater. It also extracts all trigrams starting with the current token.</li>
</ul>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">'enable-stemming'</span></code></td>
<td><p class="first">Stemming is the process of reducing inflected words to their word stem or base form. For example, word stem
of “eating” is “eat”, word stem of “backwards” is “backward”. Workbench extracts word stems using a variant
of the <a class="reference external" href="https://tartarus.org/martin/PorterStemmer/">Porter stemming algorithm</a> that only removes
inflectional suffixes.</p>
<p>If this flag is set to <code class="docutils literal"><span class="pre">True</span></code>, the stemmed versions of the n-grams are extracted from the query in
addition to regular n-grams when using the <code class="docutils literal"><span class="pre">'bag-of-words-seq'</span></code> feature described above.</p>
<p>Example:</p>
<blockquote>
<div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="s1">&#39;features&#39;</span><span class="p">:</span> <span class="p">{</span>
     <span class="s1">&#39;bag-of-words-seq&#39;</span><span class="p">:</span> <span class="p">{</span>
         <span class="s1">&#39;ngram_lengths_to_start_positions&#39;</span><span class="p">:</span> <span class="p">{</span>
             <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
         <span class="p">}</span>
     <span class="p">},</span>
     <span class="s1">&#39;enable-stemming&#39;</span><span class="p">:</span> <span class="bp">True</span>
<span class="p">}</span>
</pre></div>
</div>
</div></blockquote>
<p class="last">Given the query “{two|sys_number} orders of {breadsticks|dish}” and a classifier extracting features for
the token “of”, the above config would extract [“orders”, “of”, “breadsticks”, <strong>“order”, “breadstick”</strong>].</p>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">'char-ngrams-seq'</span></code></td>
<td><p class="first">Generates character n-grams of specified lengths from the query text
surrounding the current token.</p>
<p>Settings:</p>
<p>A dictionary with character n-gram lengths as keys
and a list of starting positions as values.
Each starting position is a token index,
relative to the current token.</p>
<p>Examples:</p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">'ngram_lengths_to_start_positions':</span> <span class="pre">{1:</span> <span class="pre">[0],</span> <span class="pre">2:</span> <span class="pre">[0]}</span></code></dt>
<dd><ul class="first last simple">
<li>extracts all characters (unigrams) and character bigrams starting with the current token</li>
</ul>
</dd>
<dt><code class="docutils literal"><span class="pre">'ngram_lengths_to_start_positions':</span> <span class="pre">{1:</span> <span class="pre">[-1,</span> <span class="pre">0,</span> <span class="pre">1],</span> <span class="pre">2:</span> <span class="pre">[-1,</span> <span class="pre">0,</span> <span class="pre">1]}</span></code></dt>
<dd><ul class="first last simple">
<li>additionally includes character unigrams and bigrams starting from the words before
and after the current token</li>
</ul>
</dd>
</dl>
<p>Given the query “weather in {Utah|location}”
and a classifier extracting features for the token “in”:</p>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">{1:</span> <span class="pre">[0]}</span></code></dt>
<dd><ul class="first last simple">
<li>extracts ‘i’, and ‘n’</li>
</ul>
</dd>
<dt><code class="docutils literal"><span class="pre">{2:</span> <span class="pre">[-1,</span> <span class="pre">0,</span> <span class="pre">1]}</span></code></dt>
<dd><ul class="first last simple">
<li>extracts ‘we’, ‘ea’, ‘at’, ‘th’, ‘he’, ‘er’, ‘in’, and ‘Ut’ ‘ta’ ‘ah’</li>
</ul>
</dd>
</dl>
<p>Additionally, you can also limit the character n-grams considered while extracting the feature by setting
a threshold on their frequency. These frequencies are computed over the entire training set. This prevents
infrequent n-grams from being used as features. By default, the threshold is set to 0.</p>
<p>Example:</p>
<blockquote class="last">
<div><div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s1">&#39;ngram_lengths_to_start_positions&#39;</span><span class="p">:</span> <span class="p">{</span><span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="mi">3</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
  <span class="s1">&#39;thresholds&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li>extracts all character bigrams in current token and previous token whose frequency in the
training set is 5 or greater. It also extracts all character trigrams in the current token.</li>
</ul>
</div></blockquote>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">'in-gaz-span-seq'</span></code></td>
<td>Generates a set of features indicating the presence of the current token in different entity gazetteers,
along with popularity information (as defined in the gazetteer).</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">'sys-candidates-seq'</span></code></td>
<td><p class="first">Generates a set of features indicating the presence of system entities in the query text surrounding the
current token.</p>
<p>Settings:</p>
<p>A dictionary with a single key named <code class="docutils literal"><span class="pre">'start_positions'</span></code> and a list of different starting positions
as its value. As in the <code class="docutils literal"><span class="pre">'bag-of-words-seq'</span></code> feature, each starting position is a token index, relative
to the the current token.</p>
<p>Example:</p>
<dl class="last docutils">
<dt><code class="docutils literal"><span class="pre">'start_positions':</span> <span class="pre">[-1,</span> <span class="pre">0,</span> <span class="pre">1]</span></code></dt>
<dd><ul class="first last simple">
<li>extracts features indicating whether the current token or its immediate neighbors are system entities</li>
</ul>
</dd>
</dl>
</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The LSTM model only supports the ‘in-gaz-span-seq’ feature since, for entity recognition tasks, it requires a minimal set of input features to achieve accuracies comparable to traditional models.</p>
</div>
<ol class="arabic simple" id="entity-tuning" start="3">
<li><strong>Hyperparameter Settings</strong></li>
</ol>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">'params'</span></code> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><code class="xref py py-class docutils literal"><span class="pre">dict</span></code></a>)</dt>
<dd><div class="first line-block">
<div class="line"><br /></div>
</div>
<p class="last">A dictionary of values to be used for model hyperparameters during training. Examples include the norm used in penalization as <code class="docutils literal"><span class="pre">'penalty'</span></code> for MEMM, the coefficients for L1 and L2 regularization <code class="docutils literal"><span class="pre">'c1'</span></code> and <code class="docutils literal"><span class="pre">'c2'</span></code> for CRF, and so on. The list of allowable hyperparameters depends on the model selected. See the <a class="reference internal" href="#er-models"><span class="std std-ref">reference links</span></a> above for parameter lists.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">'param_selection'</span></code> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><code class="xref py py-class docutils literal"><span class="pre">dict</span></code></a>)</dt>
<dd><div class="first line-block">
<div class="line"><br /></div>
</div>
<p>A dictionary of settings for <a class="reference external" href="http://scikit-learn.org/stable/modules/grid_search">hyperparameter selection</a>. Provides an alternative to the <code class="docutils literal"><span class="pre">'params'</span></code> dictionary above if the ideal hyperparameters for the model are not already known and need to be estimated.</p>
<p>To estimate parameters, Workbench needs two pieces of information from the developer:</p>
<ol class="arabic simple">
<li>The parameter space to search, as the value for the <code class="docutils literal"><span class="pre">'grid'</span></code> key</li>
<li>The strategy for splitting the labeled data into training and validation sets, as the value for the <code class="docutils literal"><span class="pre">'type'</span></code> key</li>
</ol>
<p>Depending on the splitting scheme selected, the <code class="xref py py-data docutils literal"><span class="pre">param_selection</span></code> dictionary can contain other keys that define additional settings. The table below enumerates the allowable keys.</p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="83%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Key</th>
<th class="head">Value</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">'grid'</span></code></td>
<td><p class="first">A dictionary which maps each hyperparameter to a list of potential values to search.
Here is an example for a <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression">logistic regression</a> model:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s1">&#39;penalty&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">],</span>
  <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">],</span>
   <span class="s1">&#39;fit_intercept&#39;</span><span class="p">:</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p class="last">See the <a class="reference internal" href="#er-models"><span class="std std-ref">reference links</span></a> above for details on the hyperparameters available for each model.</p>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">'type'</span></code></td>
<td><p class="first">The <a class="reference external" href="http://scikit-learn.org/stable/modules/cross_validation">cross-validation</a> methodology to use. One of:</p>
<ul class="last simple">
<li><code class="docutils literal"><span class="pre">'k-fold'</span></code>: <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold">K-folds</a></li>
<li><code class="docutils literal"><span class="pre">'shuffle'</span></code>: <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit">Randomized folds</a></li>
<li><code class="docutils literal"><span class="pre">'group-k-fold'</span></code>: <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold">K-folds with non-overlapping groups</a></li>
<li><code class="docutils literal"><span class="pre">'group-shuffle'</span></code>: <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupShuffleSplit">Group-aware randomized folds</a></li>
<li><code class="docutils literal"><span class="pre">'stratified-k-fold'</span></code>: <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold">Stratified k-folds</a></li>
<li><code class="docutils literal"><span class="pre">'stratified-shuffle'</span></code>: <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit">Stratified randomized folds</a></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">'k'</span></code></td>
<td>Number of folds (splits)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">'scoring'</span></code></td>
<td><p class="first">The metric to use for evaluating model performance. One of:</p>
<ul class="last simple">
<li><code class="docutils literal"><span class="pre">'accuracy'</span></code>: Accuracy score at a tag level</li>
<li><code class="docutils literal"><span class="pre">'seq_accuracy'</span></code>: Accuracy score at a full sequence level (not available for MEMM)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="last">To identify the parameters that give the highest accuracy, the <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method does an <a class="reference external" href="http://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search">exhaustive grid search</a> over the parameter space, evaluating candidate models using the specified cross-validation strategy. Subsequent calls to <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> can use these optimal parameters and skip the parameter selection process.</p>
</dd>
</dl>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The LSTM model does not support automatic hyperparameter tuning. The user needs to manually tune the hyperparameters for the individual datasets.</p>
</div>
<ol class="arabic simple" start="4">
<li><strong>Custom Train/Test Settings</strong></li>
</ol>
<dl class="docutils">
<dt><code class="docutils literal"><span class="pre">'train_label_set'</span></code> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><code class="xref py py-class docutils literal"><span class="pre">str</span></code></a>)</dt>
<dd><div class="first line-block">
<div class="line"><br /></div>
</div>
<p class="last">A string representing a regex pattern that selects all training files for entity model training with filenames that match the pattern. The default regex when this key is not specified is <code class="docutils literal"><span class="pre">'train.*\.txt'</span></code>.</p>
</dd>
<dt><code class="docutils literal"><span class="pre">'test_label_set'</span></code> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><code class="xref py py-class docutils literal"><span class="pre">str</span></code></a>)</dt>
<dd><div class="first line-block">
<div class="line"><br /></div>
</div>
<p class="last">A string representing a regex pattern that selects all evaluation files for entity model testing with filenames that match the pattern. The default regex when this key is not specified is <code class="docutils literal"><span class="pre">'test.*\.txt'</span></code>.</p>
</dd>
</dl>
</div>
<div class="section" id="training-with-custom-configurations">
<span id="build-entity-with-config"></span><h3>Training with custom configurations<a class="headerlink" href="#training-with-custom-configurations" title="Permalink to this headline">¶</a></h3>
<p>To override Workbench’s default entity recognizer configuration with custom settings, you can either edit the app configuration file, or, you can call the <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method with appropriate arguments.</p>
<div class="section" id="application-configuration-file">
<h4>1. Application configuration file<a class="headerlink" href="#application-configuration-file" title="Permalink to this headline">¶</a></h4>
<p>When you define custom classifier&nbsp;settings in <code class="docutils literal"><span class="pre">config.py</span></code>, the <code class="xref py py-meth docutils literal"><span class="pre">EntityRecognizer.fit()</span></code> and <code class="xref py py-meth docutils literal"><span class="pre">NaturalLanguageProcessor.build()</span></code> methods use those settings instead of Workbench’s defaults. To do this, define a dictionary of your custom settings, named <code class="xref py py-data docutils literal"><span class="pre">ENTITY_RECOGNIZER_CONFIG</span></code>.</p>
<p>Here’s an example of a <code class="docutils literal"><span class="pre">config.py</span></code> file where custom settings optimized for the app override the preset configuration for the entity recognizer.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">ENTITY_RECOGNIZER_CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;model_type&#39;</span><span class="p">:</span> <span class="s1">&#39;tagger&#39;</span><span class="p">,</span>
    <span class="s1">&#39;model_settings&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;classifier_type&#39;</span><span class="p">:</span> <span class="s1">&#39;memm&#39;</span><span class="p">,</span>
        <span class="s1">&#39;tag_scheme&#39;</span><span class="p">:</span> <span class="s1">&#39;IOBES&#39;</span><span class="p">,</span>
        <span class="s1">&#39;feature_scaler&#39;</span><span class="p">:</span> <span class="s1">&#39;max-abs&#39;</span>
    <span class="p">},</span>
    <span class="s1">&#39;param_selection&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;k-fold&#39;</span><span class="p">,</span>
        <span class="s1">&#39;k&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="s1">&#39;scoring&#39;</span><span class="p">:</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">,</span>
        <span class="s1">&#39;grid&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s1">&#39;penalty&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">],</span>
            <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10000</span><span class="p">]</span>
        <span class="p">},</span>
    <span class="p">},</span>
    <span class="s1">&#39;features&#39;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s1">&#39;bag-of-words-seq&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s1">&#39;ngram_lengths_to_start_positions&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
            <span class="p">}</span>
        <span class="p">},</span>
        <span class="s1">&#39;in-gaz-span-seq&#39;</span><span class="p">:</span> <span class="p">{},</span>
        <span class="s1">&#39;sys-candidates-seq&#39;</span><span class="p">:</span> <span class="p">{</span>
          <span class="s1">&#39;start_positions&#39;</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Settings defined in <code class="xref py py-data docutils literal"><span class="pre">ENTITY_RECOGNIZER_CONFIG</span></code> apply to entity recognizers across all domains and intents in your application. For finer-grained control, you can implement the <code class="xref py py-meth docutils literal"><span class="pre">get_entity_recognizer_config()</span></code> function in <code class="docutils literal"><span class="pre">config.py</span></code> to specify suitable configurations for each intent. This gives you the flexibility to modify models and features based on the domain and intent.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span>

<span class="k">def</span> <span class="nf">get_entity_recognizer_config</span><span class="p">(</span><span class="n">domain</span><span class="p">,</span> <span class="n">intent</span><span class="p">):</span>
    <span class="n">SPECIAL_CONFIG</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">ENTITY_RECOGNIZER_CONFIG</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">domain</span> <span class="o">==</span> <span class="s1">&#39;smart_home&#39;</span> <span class="ow">and</span> <span class="n">intent</span> <span class="o">==</span> <span class="s1">&#39;specify_location&#39;</span><span class="p">:</span>
        <span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;c1&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="s1">&#39;c2&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
            <span class="p">}</span>
        <span class="n">SPECIAL_CONFIG</span><span class="p">[</span><span class="s1">&#39;model_setting&#39;</span><span class="p">][</span><span class="s1">&#39;classifier_type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;crf&#39;</span>
        <span class="n">SPECIAL_CONFIG</span><span class="p">[</span><span class="s1">&#39;param_selection&#39;</span><span class="p">][</span><span class="s1">&#39;grid&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_grid</span>
    <span class="k">return</span> <span class="n">SPECIAL_CONFIG</span>
</pre></div>
</div>
<p>Using <code class="docutils literal"><span class="pre">config.py</span></code> is recommended for storing your optimal classifier settings once you have identified them through experimentation. Then the classifier training methods will use the optimized configuration to rebuild the models. A common use case is retraining models on newly-acquired training data, without retuning the underlying model settings.</p>
<p>Since this method requires updating a file each time you modify a setting, it’s less suitable for rapid prototyping than the method described next.</p>
</div>
<div class="section" id="arguments-to-the-fit-method">
<h4>2. Arguments to the <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method<a class="headerlink" href="#arguments-to-the-fit-method" title="Permalink to this headline">¶</a></h4>
<p>For experimenting with an&nbsp;entity recognizer, the recommended method is to use arguments to the <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method. The main areas for exploration are feature extraction, hyperparameter tuning, and model selection.</p>
<p><strong>Feature extraction</strong></p>
<p>Let’s start with the baseline classifier that was trained <a class="reference internal" href="#baseline-entity-fit"><span class="std std-ref">above</span></a>. Here’s how you get the default feature set used by the classifer.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">my_features</span> <span class="o">=</span> <span class="n">er</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">features</span>
<span class="n">my_features</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">  &#39;bag-of-words-seq&#39;: {</span>
<span class="go">    &#39;ngram_lengths_to_start_positions&#39;: {</span>
<span class="go">      1: [-2, -1, 0, 1, 2],</span>
<span class="go">      2: [-2, -1, 0, 1]</span>
<span class="go">    }</span>
<span class="go">  },</span>
<span class="go">  &#39;in-gaz-span-seq&#39;: {},</span>
<span class="go">  &#39;sys-candidates-seq&#39;: {</span>
<span class="go">    &#39;start_positions&#39;: [-1, 0, 1]</span>
<span class="go">  }</span>
<span class="go">}</span>
</pre></div>
</div>
<p>Notice that the <code class="docutils literal"><span class="pre">'ngram_lengths_to_start_positions'</span></code> settings tell the classifier to extract n-grams within a context window of two tokens or less around the token of interest —&nbsp;that is, just words in the immediate vicinity.</p>
<p>Let’s have the classifier look at a larger context window — extract n-grams starting from tokens that are further away. We’ll see whether that provides better information than the smaller default window. To do so, change the <code class="docutils literal"><span class="pre">'ngram_lengths_to_start_positions'</span></code> settings to extract all the unigrams and bigrams in a window of three tokens around the current token, as shown below.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">my_features</span><span class="p">[</span><span class="s1">&#39;bag-of-words-seq&#39;</span><span class="p">][</span><span class="s1">&#39;ngram_lengths_to_start_positions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="mi">1</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="mi">2</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="p">}</span>
<span class="n">my_features</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">  &#39;bag-of-words-seq&#39;: {</span>
<span class="go">    &#39;ngram_lengths_to_start_positions&#39;: {</span>
<span class="go">      1: [-3, -2, -1, 0, 1, 2, 3],</span>
<span class="go">      2: [-3, -2, -1, 0, 1, 2]</span>
<span class="go">    }</span>
<span class="go">  },</span>
<span class="go">  &#39;in-gaz-span-seq&#39;: {},</span>
<span class="go">  &#39;sys-candidates-seq&#39;: {</span>
<span class="go">    &#39;start_positions&#39;: [-1, 0, 1]</span>
<span class="go">  }</span>
<span class="go">}</span>
</pre></div>
</div>
<p>Suppose w<sub>i</sub> represents the word at the <em>ith</em> index in the query, where the index is calculated relative to the current token. Then, the above feature configuration should extract the following n-grams (w<sub>0</sub> being the current token).</p>
<blockquote>
<div><ul class="simple">
<li>Unigrams: { w<sub>-3</sub>, w<sub>-2</sub>, w<sub>-1</sub>, w<sub>0</sub>, w<sub>1</sub>, w<sub>2</sub>, w<sub>3</sub> }</li>
<li>Bigrams: { w<sub>-3</sub>w<sub>-2</sub>, w<sub>-2</sub>w<sub>-1</sub>, w<sub>-1</sub>w<sub>0</sub>,  w<sub>0</sub>w<sub>1</sub>, w<sub>1</sub>w<sub>2</sub>, w<sub>2</sub>w<sub>3</sub> }</li>
</ul>
</div></blockquote>
<p>To retrain the classifier with the updated feature set, pass in the <code class="xref py py-data docutils literal"><span class="pre">my_features</span></code> dictionary as an argument to the <code class="xref py py-data docutils literal"><span class="pre">features</span></code> parameter of the <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method. This trains the entity recognition model using our new feature extraction settings, while continuing to use Workbench defaults for model type (MEMM) and hyperparameter selection.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">er</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">my_features</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">Fitting entity recognizer: domain=&#39;weather&#39;, intent=&#39;check_weather&#39;</span>
<span class="go">Selecting hyperparameters using k-fold cross-validation with 5 splits</span>
<span class="go">Best accuracy: 99.04%, params: {&#39;C&#39;: 10000, &#39;penalty&#39;: &#39;l2&#39;}</span>
</pre></div>
</div>
<p>The exact accuracy number and the selected params might be different each time we run hyperparameter tuning, which we will explore in detail in the next section.</p>
<p><strong>Hyperparameter tuning</strong></p>
<p>View the model’s <a class="reference internal" href="#entity-tuning"><span class="std std-ref">hyperparameters</span></a>, keeping in mind the hyperparameters for the MEMM model in Workbench. These include: <code class="docutils literal"><span class="pre">'C'</span></code>, the inverse of regularization strength; and, <code class="docutils literal"><span class="pre">'fit_intercept'</span></code>, which determines whether to add an intercept term to the decision function. The <code class="docutils literal"><span class="pre">'fit_intercept'</span></code> parameter is not shown in the response but defaults to <code class="docutils literal"><span class="pre">'True'</span></code>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">my_param_settings</span> <span class="o">=</span> <span class="n">er</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">param_selection</span>
<span class="n">my_param_settings</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">  &#39;grid&#39;: {</span>
<span class="go">    &#39;C&#39;: [0.01, 1, 100, 10000, 1000000, 100000000],</span>
<span class="go">    &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;]</span>
<span class="go">  },</span>
<span class="go"> &#39;k&#39;: 5,</span>
<span class="go"> &#39;scoring&#39;: &#39;accuracy&#39;,</span>
<span class="go"> &#39;type&#39;: &#39;k-fold&#39;</span>
<span class="go">}</span>
</pre></div>
</div>
<p>Let’s reduce the range of values to search for <code class="docutils literal"><span class="pre">'C'</span></code>, and allow the hyperparameter estimation process to choose whether to add an intercept term to the decision function.</p>
<p>Pass the updated settings to <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> as an argument to the <code class="xref py py-data docutils literal"><span class="pre">param_selection</span></code> parameter. The <code class="xref py py-meth docutils literal"><span class="pre">fit()</span></code> method then searches over the updated parameter grid, and prints the hyperparameter values for the model whose cross-validation accuracy is highest.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">my_param_settings</span><span class="p">[</span><span class="s1">&#39;grid&#39;</span><span class="p">][</span><span class="s1">&#39;C&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10000</span><span class="p">]</span>
<span class="n">my_param_settings</span><span class="p">[</span><span class="s1">&#39;grid&#39;</span><span class="p">][</span><span class="s1">&#39;fit_intercept&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;True&#39;</span><span class="p">,</span> <span class="s1">&#39;False&#39;</span><span class="p">]</span>
<span class="n">my_param_settings</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">  &#39;grid&#39;: {</span>
<span class="go">    &#39;C&#39;: [0.01, 1, 100, 10000],</span>
<span class="go">    &#39;fit_intercept&#39;: [&#39;True&#39;, &#39;False&#39;],</span>
<span class="go">    &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;]</span>
<span class="go">  },</span>
<span class="go"> &#39;k&#39;: 5,</span>
<span class="go"> &#39;scoring&#39;: &#39;accuracy&#39;,</span>
<span class="go"> &#39;type&#39;: &#39;k-fold&#39;</span>
<span class="go">}</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">er</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">param_selection</span><span class="o">=</span><span class="n">my_param_settings</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">Fitting entity recognizer: domain=&#39;weather&#39;, intent=&#39;check_weather&#39;</span>
<span class="go">No app configuration file found. Using default entity model configuration</span>
<span class="go">Selecting hyperparameters using k-fold cross-validation with 5 splits</span>
<span class="go">Best accuracy: 99.09%, params: {&#39;C&#39;: 100, &#39;fit_intercept&#39;: &#39;False&#39;, &#39;penalty&#39;: &#39;l1&#39;}</span>
</pre></div>
</div>
<p>Finally, we’ll try a new cross-validation strategy of randomized folds, replacing the default of k-fold. We’ll keep the default of five folds. To do this, we modify the values of the   <code class="docutils literal"><span class="pre">'type'</span></code> key in <code class="xref py py-data docutils literal"><span class="pre">my_param_settings</span></code>:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">my_param_settings</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;shuffle&#39;</span>
<span class="n">my_param_settings</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">  &#39;grid&#39;: {</span>
<span class="go">    &#39;C&#39;: [0.01, 1, 100, 10000],</span>
<span class="go">    &#39;fit_intercept&#39;: [&#39;True&#39;, &#39;False&#39;],</span>
<span class="go">    &#39;penalty&#39;: [&#39;l1&#39;, &#39;l2&#39;]</span>
<span class="go">  },</span>
<span class="go"> &#39;k&#39;: 5,</span>
<span class="go"> &#39;scoring&#39;: &#39;accuracy&#39;,</span>
<span class="go"> &#39;type&#39;: &#39;shuffle&#39;</span>
<span class="go">}</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">er</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">param_selection</span><span class="o">=</span><span class="n">my_param_settings</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">Fitting entity recognizer: domain=&#39;weather&#39;, intent=&#39;check_weather&#39;</span>
<span class="go">No app configuration file found. Using default entity model configuration</span>
<span class="go">Selecting hyperparameters using shuffle cross-validation with 5 splits</span>
<span class="go">Best accuracy: 99.39%, params: {&#39;C&#39;: 100, &#39;fit_intercept&#39;: &#39;False&#39;, &#39;penalty&#39;: &#39;l1&#39;}</span>
</pre></div>
</div>
<p>For a list of configurable hyperparameters for each model, along with available cross-validation methods, see <a class="reference internal" href="#entity-tuning"><span class="std std-ref">hyperparameter settings</span></a>.</p>
<p><strong>Model settings</strong></p>
<p>To vary the model training settings, start by inspecting the current settings:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">my_model_settings</span> <span class="o">=</span> <span class="n">er</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">model_settings</span>
<span class="n">my_model_settings</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">{&#39;feature_scaler&#39;: &#39;max-abs&#39;, &#39;tag_scheme&#39;: &#39;IOB&#39;}</span>
</pre></div>
</div>
<p>For an example experiment, we’ll turn off feature scaling and change the tagging scheme to IOBES, while leaving defaults in place for feature extraction and hyperparameter selection.</p>
<p>Retrain the entity recognition model with our updated settings:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">my_model_settings</span><span class="p">[</span><span class="s1">&#39;feature_scaler&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">my_model_settings</span><span class="p">[</span><span class="s1">&#39;tag_scheme&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;IOBES&#39;</span>
<span class="n">er</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model_settings</span><span class="o">=</span><span class="n">my_model_settings</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">Fitting entity recognizer: domain=&#39;weather&#39;, intent=&#39;check_weather&#39;</span>
<span class="go">No app configuration file found. Using default entity model configuration</span>
<span class="go">Selecting hyperparameters using k-fold cross-validation with 5 splits</span>
<span class="go">Best accuracy: 98.78%, params: {&#39;C&#39;: 10000, &#39;penalty&#39;: &#39;l2&#39;}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="run-the-entity-recognizer">
<span id="predict-entities"></span><h2>Run the entity recognizer<a class="headerlink" href="#run-the-entity-recognizer" title="Permalink to this headline">¶</a></h2>
<p>Entity recognition takes place in two steps:</p>
<blockquote>
<div><ol class="arabic simple">
<li>The trained sequence labeling model predicts the output tag (in IOB or IOBES format) with the highest probability for each token in the input query.</li>
<li>The predicted tags are then processed to extract the span and type of each entity in the query.</li>
</ol>
</div></blockquote>
<p>Run the trained entity recognizer on a test query using the <code class="xref py py-meth docutils literal"><span class="pre">EntityRecognizer.predict()</span></code> method, which returns a list of detected entities in the query.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">er</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s1">&#39;Weather in San Francisco next week&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">(&lt;QueryEntity &#39;San Francisco&#39; (&#39;city&#39;) char: [11-23], tok: [2-3]&gt;,</span>
<span class="go"> &lt;QueryEntity &#39;next week&#39; (&#39;sys_time&#39;) char: [25-33], tok: [4-5]&gt;)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">At runtime, the natural language processor’s <code class="xref py py-meth docutils literal"><span class="pre">process()</span></code> method calls <code class="xref py py-meth docutils literal"><span class="pre">predict()</span></code> to recognize all the entities in an incoming query.</p>
</div>
<p>We want to know how confident our trained model is in its prediction. To view the confidence score of the predicted entity label, use the <code class="xref py py-meth docutils literal"><span class="pre">EntityRecognizer.predict_proba()</span></code> method. This is useful both for experimenting with the classifier settings and for debugging classifier performance.</p>
<p>The result is a tuple of tuples whose first element is the entity itself and second element is the associated confidence score.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">er</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="s1">&#39;Weather in San Francisco next week&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">((&lt;QueryEntity &#39;San Francisco&#39; (&#39;city&#39;) char: [11-23], tok: [2-3]&gt;, 0.9994949555840245),</span>
<span class="go">(&lt;QueryEntity &#39;next week&#39; (&#39;sys_time&#39;) char: [25-33], tok: [4-5]&gt;, 0.9994573416716696))</span>
</pre></div>
</div>
<p>An ideal entity recognizer would assign a high confidence score to the expected (correct) class label for a test query, while assigning very low probabilities to incorrect labels.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Unlike the domain and intent labels, the confidence score reported for an entity sequence is the score associated with the least likely tag in that sequence. For example, the model assigns the tag <code class="docutils literal"><span class="pre">'B|city'</span></code> to the word “San” with some score x and  <code class="docutils literal"><span class="pre">'I|city'</span></code> to the word “Francisco” with some score y. The final confidence score associated with this entity is the minimum of x and y.</p>
</div>
<p>The <code class="xref py py-meth docutils literal"><span class="pre">predict()</span></code> and <code class="xref py py-meth docutils literal"><span class="pre">predict_proba()</span></code> methods take one query at a time. Next, we’ll see how to test a trained model on a batch of labeled test queries.</p>
</div>
<div class="section" id="evaluate-classifier-performance">
<span id="entity-evaluation"></span><h2>Evaluate classifier performance<a class="headerlink" href="#evaluate-classifier-performance" title="Permalink to this headline">¶</a></h2>
<p>Before you can evaluate the accuracy of your trained entity recognizer, you must first create labeled test data and place it in your Workbench project as described in the <a class="reference internal" href="nlp.html#evaluate-nlp"><span class="std std-ref">Natural Language Processor</span></a> chapter.</p>
<p>Then, when you are ready, use the <code class="xref py py-meth docutils literal"><span class="pre">EntityRecognizer.evaluate()</span></code> method, which</p>
<blockquote>
<div><ul class="simple">
<li>strips away all ground truth annotations from the test queries,</li>
<li>passes the resulting unlabeled queries to the trained entity recognizer for prediction, and</li>
<li>compares the classifier’s output predictions against the ground truth labels to compute the model’s prediction accuracy.</li>
</ul>
</div></blockquote>
<p>In the example below, the model gets 35 out of 37 test queries correct, resulting in an accuracy of about 94.6%.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">er</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">Loading queries from file weather/check_weather/test.txt</span>
<span class="go">&lt;EntityModelEvaluation score: 94.59%, 35 of 37 examples correct&gt;</span>
</pre></div>
</div>
<p>Note that this is <em>query-level</em> accuracy. A prediction on a query can only be graded as “correct” when all the entities detected by the entity recognizer exactly match exactly the annotated entities in the test query.</p>
<p>The aggregate accuracy score we see above is only the beginning, because the <code class="xref py py-meth docutils literal"><span class="pre">evaluate()</span></code> method returns a rich object containing overall statistics, statistics by class, a confusion matrix, and sequence statistics.</p>
<p>Print all the model performance statistics reported by the <code class="xref py py-meth docutils literal"><span class="pre">evaluate()</span></code> method:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="nb">eval</span> <span class="o">=</span> <span class="n">er</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="nb">eval</span><span class="o">.</span><span class="n">print_stats</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">Overall tag-level statistics:</span>

<span class="go">   accuracy f1_weighted          tp          tn          fp          fn    f1_macro    f1_micro</span>
<span class="go">      0.986       0.985         204         825           3           3       0.975       0.986</span>



<span class="go">Tag-level statistics by class:</span>

<span class="go">              class      f_beta   precision      recall     support          tp          tn          fp          fn</span>
<span class="go">                 O|       0.990       0.981       1.000         155         155          49           3           0</span>
<span class="go">             B|city       0.985       1.000       0.971          34          33         173           0           1</span>
<span class="go">         B|sys_time       1.000       1.000       1.000           4           4         203           0           0</span>
<span class="go">         I|sys_time       1.000       1.000       1.000           3           3         204           0           0</span>
<span class="go">             I|city       0.900       1.000       0.818          11           9         196           0           2</span>



<span class="go">Confusion matrix:</span>

<span class="go">                           O|         B|city     B|sys_time     I|sys_time         I|city</span>
<span class="go">            O|            155              0              0              0              0</span>
<span class="go">        B|city              1             33              0              0              0</span>
<span class="go">    B|sys_time              0              0              4              0              0</span>
<span class="go">    I|sys_time              0              0              0              3              0</span>
<span class="go">        I|city              2              0              0              0              9</span>



<span class="go">Segment-level statistics:</span>

<span class="go">         le          be         lbe          tp          tn          fp          fn</span>
<span class="go">          0           1           0          36          42           0           1</span>



<span class="go">Sequence-level statistics:</span>

<span class="go">  sequence_accuracy</span>
<span class="go">              0.946</span>
</pre></div>
</div>
<p>The <code class="xref py py-meth docutils literal"><span class="pre">eval.get_stats()</span></code> method returns all the above statistics in a structured dictionary without printing them to the console.</p>
<p>Let’s decipher the statistics output by the <code class="xref py py-meth docutils literal"><span class="pre">evaluate()</span></code> method.</p>
<dl class="docutils">
<dt><strong>Overall tag-level statistics</strong></dt>
<dd><div class="first line-block">
<div class="line"><br /></div>
</div>
<p>Aggregate IOB or IOBES tag-level stats measured across the entire test set:</p>
<table border="1" class="docutils">
<colgroup>
<col width="12%" />
<col width="88%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>accuracy</td>
<td><a class="reference external" href="http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score">Classification accuracy score</a></td>
</tr>
<tr class="row-even"><td>f1_weighted</td>
<td><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">Class-weighted average f1 score</a></td>
</tr>
<tr class="row-odd"><td>tp</td>
<td>Number of <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">true positives</a></td>
</tr>
<tr class="row-even"><td>tn</td>
<td>Number of <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">true negatives</a></td>
</tr>
<tr class="row-odd"><td>fp</td>
<td>Number of <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">false positives</a></td>
</tr>
<tr class="row-even"><td>fn</td>
<td>Number of <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">false negatives</a></td>
</tr>
<tr class="row-odd"><td>f1_macro</td>
<td><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">Macro-averaged f1 score</a></td>
</tr>
<tr class="row-even"><td>f1_micro</td>
<td><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html">Micro-averaged f1 score</a></td>
</tr>
</tbody>
</table>
<p>When interpreting these statistics, consider whether your app and evaluation results fall into one of the cases below, and if so, apply the accompanying guideline. This list is basic, not exhaustive, but should get you started.</p>
<ul class="last simple">
<li><strong>Classes are balanced</strong> – When the number of annotated entities for each entity type are comparable and each entity type is equally important, focusing on the accuracy metric is usually good enough. For entity recognition it is very unlikely that your data would fall into this category, since the O tag (used for words that are not part of an entity) usually occurs much more often than the I/B/E/S tags (for words that are part of an entity).</li>
<li><strong>Classes are imbalanced</strong> — In this case, it’s important to take the f1 scores into account. For entity recognition it is also important to consider the segment level statistics described below. By primarily optimizing for f1, your model will tend to predict no entity rather than predict one that is uncertain about. See <a class="reference external" href="https://nlpers.blogspot.com/2006/08/doing-named-entity-recognition-dont.html">this blog post</a>.</li>
<li><strong>All f1 and accuracy scores are low</strong> — When entity recognition is performing poorly across all entity types, either of the following may be the problem: 1) You do not have enough training data for the model to learn, or 2) you need to tune your model hyperparameters. Look at segment-level statistics for a more intuitive breakdown of where the model is making errors.</li>
<li><strong>f1 weighted is higher than f1 macro</strong> — This means that entity types with fewer evaluation examples are performing poorly. Try adding more data to these entity types. This entails adding more training queries with labeled entities, specifically entities of the type that are performing the worst as indicated in the tag-level statistics table.</li>
<li><strong>f1 macro is higher than f1 weighted</strong> — This means that entity types with more evaluation examples are performing poorly. Verify that the number of evaluation examples reflects the class distribution of your training examples.</li>
<li><strong>f1 micro is higher than f1 macro</strong> — This means that certain entity types are being misclassified more often than others. Identify the problematic entity types by checking the tag-level class-wise statistics below. Some entity types may be too similar to others, or you may need to add more training data.</li>
<li><strong>Some classes are more important than others</strong> — If some entities are more important than others for your use case, it is best to focus especially on the tag-level class-wise statistics below.</li>
</ul>
</dd>
<dt><strong>Tag-level statistics by class</strong></dt>
<dd><div class="first line-block">
<div class="line"><br /></div>
</div>
<p>Tag-level (IOB or IOBES) statistics that are calculated for each class:</p>
<table border="1" class="last docutils">
<colgroup>
<col width="12%" />
<col width="88%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>class</td>
<td>Entity tag (in IOB or IOBES format)</td>
</tr>
<tr class="row-even"><td>f_beta</td>
<td><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score">F-beta score</a></td>
</tr>
<tr class="row-odd"><td>precision</td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall#Precision">Precision</a></td>
</tr>
<tr class="row-even"><td>recall</td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall#Recall">Recall</a></td>
</tr>
<tr class="row-odd"><td>support</td>
<td>Number of test entities with this entity tag (based on ground truth)</td>
</tr>
<tr class="row-even"><td>tp</td>
<td>Number of <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">true positives</a></td>
</tr>
<tr class="row-odd"><td>tn</td>
<td>Number of <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">true negatives</a></td>
</tr>
<tr class="row-even"><td>fp</td>
<td>Number of <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">false positives</a></td>
</tr>
<tr class="row-odd"><td>fn</td>
<td>Number of <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">false negatives</a></td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Confusion matrix</strong></dt>
<dd><div class="first line-block">
<div class="line"><br /></div>
</div>
<p class="last">A <a class="reference external" href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a> where each row represents the number of instances in an actual class and each column represents the number of instances in a predicted class. This reveals whether the classifier tends to confuse two classes, i.e., mislabel one tag as another.</p>
</dd>
<dt><strong>Segment-level statistics</strong></dt>
<dd><div class="first line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Currently, segment-level statistics cannot be generated for the IOBES tag scheme. They are only available for IOB.</p>
</div>
<p>Although it is useful to analyze tag-level statistics, they don’t tell the full story for entity recognition in an intuitive way. It helps to think of the entity recognizer as performing two tasks: 1) identifying the span of words that should be part of an entity, and 2) selecting the label for the identified entity. When the recognizer makes a mistake, it misidentifies either the label, the span boundary, or both.</p>
<p>Segment-level statistics capture the distribution of these error types across all the segments in a query.</p>
<p>A segment is either:</p>
<blockquote>
<div><ul class="simple">
<li>A continuous span of non-entity tokens, or</li>
<li>A continuous span of tokens that represents a single entity</li>
</ul>
</div></blockquote>
<p>For example, the query “I’ll have an {eggplant parm|dish} and some {breadsticks|dish} please” has five segments: “I’ll have an”, “eggplant parm”, “and some”, “breadsticks”, and “please”.</p>
<p>The table below describes the segment-level statistics available in Workbench.</p>
<table border="1" class="docutils">
<colgroup>
<col width="3%" />
<col width="6%" />
<col width="91%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>Abbreviation</td>
<td>Statistic</td>
<td>Description</td>
</tr>
<tr class="row-even"><td>le</td>
<td><strong>Label error</strong></td>
<td>The classifier correctly predicts the existence of an entity and the span of that entity, but chooses the wrong label. For example, the classifier recognizes that ‘pad thai’ is an entity in the query ‘Order some pad thai’, but labels it as a restaurant entity instead of a dish entity.</td>
</tr>
<tr class="row-odd"><td>be</td>
<td><strong>Boundary error</strong></td>
<td>The classifier correctly predicts the existence of an entity and its label but misclassifies its span. For example, the classifier predicts that ‘some pad thai’ is a dish entity instead of just ‘pad thai’ in the query ‘Order some pad thai’.</td>
</tr>
<tr class="row-even"><td>lbe</td>
<td><strong>Label-boundary error</strong></td>
<td>The classifier correctly predicts the existence of an entity, but gets both the label and the span wrong. For example, the classifier labels ‘some pad thai’ as an option in the query ‘Order some pad thai’. The option label is wrong (dish is correct), and, the boundary is misplaced (because it includes the word ‘some’ which does not belong in the entity).</td>
</tr>
<tr class="row-odd"><td>tp</td>
<td><strong>True positive</strong></td>
<td>The classifier correctly predicts an entity, its label, and its span.</td>
</tr>
<tr class="row-even"><td>tn</td>
<td><strong>True negative</strong></td>
<td>The classifier correctly predicts that that a segment contains no entities. For example, the classifier predicts that the query ‘Hi there’ has no entities.</td>
</tr>
<tr class="row-odd"><td>fp</td>
<td><strong>False positive</strong></td>
<td>The classifier predicts the existence of an entity that is not there. For example, the classifier predicts that ‘there’ is a dish entity in the query ‘Hi there’.</td>
</tr>
<tr class="row-even"><td>fn</td>
<td><strong>False negative</strong></td>
<td>The classifier fails to predict an entity that <em>is</em> present. For example,  the classifier predicts no entity in the query ‘Order some pad thai’.</td>
</tr>
</tbody>
</table>
<p>Note that the true positive, true negative, false positive, and false negative values are different when calculated at a segment level rather than a tag level. To illustrate this difference consider the following example:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>         I’ll  have  an      eggplant  parm    please
Exp:     O.    O     O       B|dish    I|dish  O
Pred:    O.    O.    B|dish  I|dish.   O.      O
</pre></div>
</div>
<p>In the traditional tag-level statistics, predicting <code class="docutils literal"><span class="pre">B|dish</span></code> instead of <code class="docutils literal"><span class="pre">O</span></code> and predicting <code class="docutils literal"><span class="pre">I|dish</span></code> instead of <code class="docutils literal"><span class="pre">B|dish</span></code> would both be <cite>false positives</cite>. There would also be <cite>3 true negatives</cite> for correctly predicting <code class="docutils literal"><span class="pre">O</span></code>.</p>
<p>At the segment level, however, this would be just <cite>2 true negatives</cite> (one for the segment ‘I’ll have’ and one for the segment ‘please’), and <cite>1 label-boundary error</cite> (for the segment ‘an eggplant parm’).</p>
<p class="last">Considering errors at a segment level is often more intuitive and may even provide better metrics to optimize against, as described <a class="reference external" href="https://nlpers.blogspot.com/2006/08/doing-named-entity-recognition-dont.html">here</a>.</p>
</dd>
<dt><strong>Sequence-level Statistics</strong></dt>
<dd><div class="first line-block">
<div class="line"><br /></div>
</div>
<p class="last">In Workbench, we define <em>sequence-level accuracy</em> as the fraction of queries for which the entity recognizer successfully identified <strong>all</strong> the expected entities.</p>
</dd>
</dl>
<p>Now we have a wealth of information about the performance of our classifier. Let’s go further and inspect the classifier’s predictions at the level of individual queries, to better understand error patterns.</p>
<p>View the classifier predictions for the entire test set using the <code class="xref py py-attr docutils literal"><span class="pre">results</span></code> attribute of the returned <a class="reference external" href="https://docs.python.org/3/library/functions.html#eval" title="(in Python v3.7)"><code class="xref py py-obj docutils literal"><span class="pre">eval</span></code></a> object. Each result is an instance of the <code class="xref py py-class docutils literal"><span class="pre">EvaluatedExample</span></code> class which contains information about the original input query, the expected ground truth label, the predicted label, and the predicted probability distribution over all the class labels.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="nb">eval</span><span class="o">.</span><span class="n">results</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">[</span>
<span class="go">  EvaluatedExample(example=&lt;Query &#39;check temperature outside&#39;&gt;, expected=(), predicted=(), probas=None, label_type=&#39;entities&#39;),</span>
<span class="go">  EvaluatedExample(example=&lt;Query &#39;check temperature in miami&#39;&gt;, expected=(&lt;QueryEntity &#39;miami&#39; (&#39;city&#39;) char: [21-25], tok: [3-3]&gt;,), predicted=(&lt;QueryEntity &#39;miami&#39; (&#39;city&#39;) char: [21-25], tok: [3-3]&gt;,), probas=None, label_type=&#39;entities&#39;),</span>
<span class="go">  ...</span>
<span class="go">]</span>
</pre></div>
</div>
<p>Next, we look selectively at just the correct or incorrect predictions.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="nb">eval</span><span class="o">.</span><span class="n">correct_results</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">[</span>
<span class="go">  EvaluatedExample(example=&lt;Query &#39;check temperature outside&#39;&gt;, expected=(), predicted=(), probas=None, label_type=&#39;entities&#39;),</span>
<span class="go">  EvaluatedExample(example=&lt;Query &#39;check temperature in miami&#39;&gt;, expected=(&lt;QueryEntity &#39;miami&#39; (&#39;city&#39;) char: [21-25], tok: [3-3]&gt;,), predicted=(&lt;QueryEntity &#39;miami&#39; (&#39;city&#39;) char: [21-25], tok: [3-3]&gt;,), probas=None, label_type=&#39;entities&#39;),</span>
<span class="go">  ...</span>
<span class="go">]</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="nb">eval</span><span class="o">.</span><span class="n">incorrect_results</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">[</span>
<span class="go">  EvaluatedExample(example=&lt;Query &#39;taipei current temperature&#39;&gt;, expected=(&lt;QueryEntity &#39;taipei&#39; (&#39;city&#39;) char: [0-5], tok: [0-0]&gt;,), predicted=(), probas=None, label_type=&#39;entities&#39;),</span>
<span class="go">  EvaluatedExample(example=&lt;Query &#39;london weather&#39;&gt;, expected=(&lt;QueryEntity &#39;london&#39; (&#39;city&#39;) char: [0-5], tok: [0-0]&gt;,), predicted=(), probas=None, label_type=&#39;entities&#39;)</span>
<span class="go">]</span>
</pre></div>
</div>
<p>Slicing and dicing these results for error analysis is easily done with <a class="reference external" href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions">list comprehensions</a>.</p>
<p>A simple example of this is to inspect incorrect predictions where the query’s first entity is supposed to be of a particular type. For the <code class="docutils literal"><span class="pre">city</span></code> type, we get:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="p">[(</span><span class="n">r</span><span class="o">.</span><span class="n">example</span><span class="p">,</span> <span class="n">r</span><span class="o">.</span><span class="n">expected</span><span class="p">,</span> <span class="n">r</span><span class="o">.</span><span class="n">predicted</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">eval</span><span class="o">.</span><span class="n">incorrect_results</span><span class="p">()</span> <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">expected</span> <span class="ow">and</span> <span class="n">r</span><span class="o">.</span><span class="n">expected</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">entity</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;city&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">[</span>
<span class="go">  (</span>
<span class="go">    &lt;Query &#39;taipei current temperature&#39;&gt;,</span>
<span class="go">    (&lt;QueryEntity &#39;taipei&#39; (&#39;city&#39;) char: [0-5], tok: [0-0]&gt;,),</span>
<span class="go">    ()</span>
<span class="go">  ),</span>
<span class="go">  (</span>
<span class="go">    &lt;Query &#39;london weather&#39;&gt;,</span>
<span class="go">    (&lt;QueryEntity &#39;london&#39; (&#39;city&#39;) char: [0-5], tok: [0-0]&gt;,),</span>
<span class="go">    ()</span>
<span class="go">  ),</span>
<span class="go">  (</span>
<span class="go">    &lt;Query &#39;temperature in san fran&#39;&gt;,</span>
<span class="go">    (&lt;QueryEntity &#39;san fran&#39; (&#39;city&#39;) char: [15-22], tok: [2-3]&gt;,),</span>
<span class="go">    (&lt;QueryEntity &#39;san&#39; (&#39;city&#39;) char: [15-17], tok: [2-2]&gt;,)</span>
<span class="go">  ),</span>
<span class="go">  (</span>
<span class="go">    &lt;Query &quot;how&#39;s the weather in the big apple&quot;&gt;,</span>
<span class="go">    (&lt;QueryEntity &#39;big apple&#39; (&#39;city&#39;) char: [25-33], tok: [5-6]&gt;,),</span>
<span class="go">    ()</span>
<span class="go">  )</span>
<span class="go">]</span>
</pre></div>
</div>
<p>The entity recognizer was unable to correctly detect the full <code class="docutils literal"><span class="pre">city</span></code> entity in <em>any</em> of the above queries. This is usually a sign that the training data lacks coverage for queries with language patterns or entities like those in the examples above. It could also mean that the gazetteer for this entity type is not comprehensive enough.</p>
<p>Start by looking for similar queries in the <a class="reference internal" href="../blueprints/home_assistant.html"><span class="doc">training data</span></a>. You should discover that the <code class="docutils literal"><span class="pre">check_weather</span></code> intent does indeed lack labeled training queries like the first two queries above.</p>
<p>To solve this problem, you could try adding more queries annotated with the <code class="docutils literal"><span class="pre">city</span></code> entity to the <code class="docutils literal"><span class="pre">check_weather</span></code> intent’s training data. Then, the recognition model should be able to generalize better.</p>
<p>The last two misclassified queries feature nicknames (<code class="docutils literal"><span class="pre">'san</span> <span class="pre">fran'</span></code> and <code class="docutils literal"><span class="pre">'the</span> <span class="pre">big</span> <span class="pre">apple'</span></code>) rather than formal city names. Noticing this, the logical step is to inspect the <a class="reference internal" href="../blueprints/home_assistant.html"><span class="doc">gazetteer data</span></a>. You should discover that this gazetteer does indeed lack slang terms and nicknames for cities.</p>
<p>To mitigate this, try expanding the <code class="docutils literal"><span class="pre">city</span></code> gazetteer to contain entries like “San Fran”, “Big Apple” and other popular synonyms for location names that are relevant to the <code class="docutils literal"><span class="pre">weather</span></code> domain.</p>
<p>Error analysis on the results of the <code class="xref py py-meth docutils literal"><span class="pre">evaluate()</span></code> method can inform your experimentation and help in building better models. Augmenting training data and adding gazetteer entries should be the first steps, as in the above example. Beyond that, you can experiment with different model types, features, and hyperparameters, as described <a class="reference internal" href="#build-entity-with-config"><span class="std std-ref">earlier</span></a> in this chapter.</p>
</div>
<div class="section" id="viewing-features-extracted-for-entity-recognition">
<h2>Viewing features extracted for entity recognition<a class="headerlink" href="#viewing-features-extracted-for-entity-recognition" title="Permalink to this headline">¶</a></h2>
<p>While training a new model or investigating a misclassification by the classifier, it is sometimes useful to view the extracted features to make sure they are as expected. For example, there may be non-ASCII characters in the query that are treated differently by the feature extractors. Or the value assigned to a particular feature may be computed differently than you expected. Not extracting the right features could lead to misclassifications. In the example below, we view the features extracted for the query ‘set alarm for 7 am’ using <code class="xref py py-meth docutils literal"><span class="pre">EntityRecognizer.view_extracted_features()</span></code> method.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">er</span><span class="o">.</span><span class="n">view_extracted_features</span><span class="p">(</span><span class="s2">&quot;set alarm for 7 am&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">[{&#39;bag_of_words|length:1|word_pos:-1&#39;: &#39;&lt;$&gt;&#39;, &#39;bag_of_words|length:1|word_pos:0&#39;: &#39;set&#39;, &#39;bag_of_words|length:1|word_pos:1&#39;: &#39;alarm&#39;, &#39;bag_of_words|length:2|word_pos:-1&#39;: &#39;&lt;$&gt; set&#39;, &#39;bag_of_words|length:2|word_pos:0&#39;: &#39;set alarm&#39;, &#39;bag_of_words|length:2|word_pos:1&#39;: &#39;alarm for&#39;},</span>
<span class="go"> {&#39;bag_of_words|length:1|word_pos:-1&#39;: &#39;set&#39;, &#39;bag_of_words|length:1|word_pos:0&#39;: &#39;alarm&#39;, &#39;bag_of_words|length:1|word_pos:1&#39;: &#39;for&#39;, &#39;bag_of_words|length:2|word_pos:-1&#39;: &#39;set alarm&#39;, &#39;bag_of_words|length:2|word_pos:0&#39;: &#39;alarm for&#39;, &#39;bag_of_words|length:2|word_pos:1&#39;: &#39;for 0&#39;},</span>
<span class="go"> {&#39;bag_of_words|length:1|word_pos:-1&#39;: &#39;alarm&#39;, &#39;bag_of_words|length:1|word_pos:0&#39;: &#39;for&#39;, &#39;bag_of_words|length:1|word_pos:1&#39;: &#39;0&#39;, &#39;bag_of_words|length:2|word_pos:-1&#39;: &#39;alarm for&#39;, &#39;bag_of_words|length:2|word_pos:0&#39;: &#39;for 0&#39;, &#39;bag_of_words|length:2|word_pos:1&#39;: &#39;0 am&#39;, &#39;sys_candidate|type:sys_time|granularity:hour|pos:1&#39;: 1, &#39;sys_candidate|type:sys_time|granularity:hour|pos:1|log_len&#39;: 1.3862943611198906},</span>
<span class="go"> {&#39;bag_of_words|length:1|word_pos:-1&#39;: &#39;for&#39;, &#39;bag_of_words|length:1|word_pos:0&#39;: &#39;0&#39;, &#39;bag_of_words|length:1|word_pos:1&#39;: &#39;am&#39;, &#39;bag_of_words|length:2|word_pos:-1&#39;: &#39;for 0&#39;, &#39;bag_of_words|length:2|word_pos:0&#39;: &#39;0 am&#39;, &#39;bag_of_words|length:2|word_pos:1&#39;: &#39;am &lt;$&gt;&#39;, &#39;sys_candidate|type:sys_time|granularity:hour|pos:0&#39;: 1, &#39;sys_candidate|type:sys_time|granularity:hour|pos:0|log_len&#39;: 1.3862943611198906, &#39;sys_candidate|type:sys_time|granularity:hour|pos:1&#39;: 1, &#39;sys_candidate|type:sys_time|granularity:hour|pos:1|log_len&#39;: 1.3862943611198906},</span>
<span class="go"> {&#39;bag_of_words|length:1|word_pos:-1&#39;: &#39;0&#39;, &#39;bag_of_words|length:1|word_pos:0&#39;: &#39;am&#39;, &#39;bag_of_words|length:1|word_pos:1&#39;: &#39;&lt;$&gt;&#39;, &#39;bag_of_words|length:2|word_pos:-1&#39;: &#39;0 am&#39;, &#39;bag_of_words|length:2|word_pos:0&#39;: &#39;am &lt;$&gt;&#39;, &#39;bag_of_words|length:2|word_pos:1&#39;: &#39;&lt;$&gt; &lt;$&gt;&#39;, &#39;sys_candidate|type:sys_time|granularity:hour|pos:-1&#39;: 1, &#39;sys_candidate|type:sys_time|granularity:hour|pos:-1|log_len&#39;: 1.3862943611198906, &#39;sys_candidate|type:sys_time|granularity:hour|pos:0&#39;: 1, &#39;sys_candidate|type:sys_time|granularity:hour|pos:0|log_len&#39;: 1.3862943611198906}]</span>
</pre></div>
</div>
<p>This is especially useful when you are writing <a class="reference internal" href="custom_features.html"><span class="doc">custom feature extractors</span></a> to inspect whether the right features are being extracted.</p>
</div>
<div class="section" id="save-model-for-future-use">
<h2>Save model for future use<a class="headerlink" href="#save-model-for-future-use" title="Permalink to this headline">¶</a></h2>
<p>Save the trained entity recognizer for later use by calling the <code class="xref py py-meth docutils literal"><span class="pre">EntityRecognizer.dump()</span></code> method. The <code class="xref py py-meth docutils literal"><span class="pre">dump()</span></code> method serializes the trained model as a <a class="reference external" href="https://docs.python.org/3/library/pickle.html">pickle file</a> and saves it to the specified location on disk.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">er</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s1">&#39;experiments/entity_recognizer.pkl&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">Saving entity recognizer: domain=&#39;weather&#39;, intent=&#39;check_weather&#39;</span>
</pre></div>
</div>
<p>You can load the saved model anytime using the <code class="xref py py-meth docutils literal"><span class="pre">EntityRecognizer.load()</span></code> method.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">er</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s1">&#39;experiments/entity_recognizer.pkl&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">Loading entity recognizer: domain=&#39;weather&#39;, intent=&#39;check_weather&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="more-about-system-entities">
<span id="system-entities"></span><h2>More about system entities<a class="headerlink" href="#more-about-system-entities" title="Permalink to this headline">¶</a></h2>
<p>System entities are generic application-agnostic entities that all Workbench applications detect automatically. There is no need to train models to learn system entities; they just work.</p>
<p>Supported system entities are enumerated in the table below.</p>
<table border="1" class="docutils">
<colgroup>
<col width="30%" />
<col width="70%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">System Entity</th>
<th class="head">Examples</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>sys_time</td>
<td>“today” , “Tuesday, Feb 18” , “last week” , “Mother’s
day”</td>
</tr>
<tr class="row-odd"><td>sys_interval</td>
<td>“tomorrow morning” , “from 9:30 - 11:00 on tuesday” ,
“Friday 13th evening”</td>
</tr>
<tr class="row-even"><td>sys_duration</td>
<td>“2 hours” , “half an hour” , “15 minutes”</td>
</tr>
<tr class="row-odd"><td>sys_temperature</td>
<td>“64°F” , “71° Fahrenheit” , “twenty seven celsius”</td>
</tr>
<tr class="row-even"><td>sys_number</td>
<td>“fifteen” , “0.62” , “500k” , “66”</td>
</tr>
<tr class="row-odd"><td>sys_ordinal</td>
<td>“3rd” , “fourth” , “first”</td>
</tr>
<tr class="row-even"><td>sys_distance</td>
<td>“10 miles” , “2feet” , “0.2 inches” , “3’’ “5km” ,”12cm”</td>
</tr>
<tr class="row-odd"><td>sys_volume</td>
<td>“500 ml” , “5liters” , “2 gallons”</td>
</tr>
<tr class="row-even"><td>sys_amount-of-money</td>
<td>“forty dollars” , “9 bucks” , “$30”</td>
</tr>
<tr class="row-odd"><td>sys_email</td>
<td>“<a class="reference external" href="mailto:help&#37;&#52;&#48;cisco&#46;com">help<span>&#64;</span>cisco<span>&#46;</span>com</a>”</td>
</tr>
<tr class="row-even"><td>sys_url</td>
<td>“washpo.com/info” , “foo.com/path/path?ext=%23&amp;foo=bla” ,
“localhost”</td>
</tr>
<tr class="row-odd"><td>sys_phone-number</td>
<td>“+91 736 124 1231” , “+33 4 76095663” , “(626)-756-4757
ext 900”</td>
</tr>
</tbody>
</table>
<p>Workbench does not assume that any of the system entities are needed in your app. It is the system entities <em>that you annotate in your training data</em> that Workbench knows are needed.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<blockquote>
<div>Workbench defines <code class="docutils literal"><span class="pre">sys_time</span></code> and <code class="docutils literal"><span class="pre">sys_interval</span></code> as subtly different entities.</div></blockquote>
<div class="last line-block">
<div class="line">The <code class="docutils literal"><span class="pre">sys_time</span></code> entity connotes a <em>value of a single unit of time</em>, where the unit can be a date, an hour, a week, and so on. For example, “tomorrow” is a <code class="docutils literal"><span class="pre">sys_time</span></code> entity because it corresponds to a single (unit) date, like “2017-07-08.”</div>
<div class="line"><br /></div>
<div class="line">The <code class="docutils literal"><span class="pre">sys_interval</span></code> entity connotes a <em>time interval</em> that <em>spans several units</em> of time. For example, “tomorrow morning” is a <code class="docutils literal"><span class="pre">sys_interval</span></code> entity because “morning” corresponds to the span of hours from 4 am to 12 pm.</div>
</div>
</div>
<div class="section" id="custom-entities-system-entities-and-training-set-size">
<h3>Custom entities, system entities, and training set size<a class="headerlink" href="#custom-entities-system-entities-and-training-set-size" title="Permalink to this headline">¶</a></h3>
<p>Any application’s training set must focus on capturing all the entity variations and language patterns for the <em>custom entities</em> that the app uses. By contrast, the part of the training set concerned with <em>system entities</em> can be relatively minimal, because Workbench does not need to train an entity recognition model to recognize system entities.</p>
</div>
<div class="section" id="annotating-system-entities">
<h3>Annotating system entities<a class="headerlink" href="#annotating-system-entities" title="Permalink to this headline">¶</a></h3>
<p>Assuming that you have defined the <a class="reference internal" href="../quickstart/03_define_the_hierarchy.html#model-hierarchy"><span class="std std-ref">domain-intent-entity-role hierarchy</span></a> for your app, you know</p>
<blockquote>
<div><ul class="simple">
<li>which system entities your app needs to use</li>
<li>what roles (if any) apply to those system entities</li>
</ul>
</div></blockquote>
<p>Use this knowledge to guide you in annotating any system entities in your training data.</p>
<p>These examples of annotated system entities come from the Home Assistant blueprint application:</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>- adjust the temperature to {65|sys_temperature}
- {in the morning|sys_interval} set the temperature to {72|sys_temperature}
- change my {6:45|sys_time|old_time} alarm to {7 am|sys_time|new_time}
- move my {6 am|sys_time|old_time} alarm to {3pm in the afternoon|sys_time|new_time}
- what&#39;s the forecast for {tomorrow afternoon|sys_interval}
</pre></div>
</div>
<p>For more examples, see the training data for any of the blueprint apps.</p>
</div>
<div class="section" id="inspecting-how-workbench-detects-system-entities">
<h3>Inspecting how Workbench detects system entities<a class="headerlink" href="#inspecting-how-workbench-detects-system-entities" title="Permalink to this headline">¶</a></h3>
<p>To see which token spans in a query are detected as system entities, and what system entities Workbench thinks they are, use the <code class="xref py py-func docutils literal"><span class="pre">parse_numerics()</span></code> function:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindmeld.ser</span> <span class="kn">import</span> <span class="n">parse_numerics</span>
<span class="n">parse_numerics</span><span class="p">(</span><span class="s2">&quot;tomorrow morning at 9am&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">([{&#39;body&#39;: &#39;tomorrow morning&#39;,</span>
<span class="go">   &#39;dim&#39;: &#39;time&#39;,</span>
<span class="go">   &#39;end&#39;: 16,</span>
<span class="go">   &#39;latent&#39;: False,</span>
<span class="go">   &#39;start&#39;: 0,</span>
<span class="go">   &#39;value&#39;: {&#39;from&#39;: {&#39;grain&#39;: &#39;hour&#39;,</span>
<span class="go">                      &#39;value&#39;: &#39;2019-01-12T04:00:00.000-08:00&#39;},</span>
<span class="go">             &#39;to&#39;: {&#39;grain&#39;: &#39;hour&#39;,</span>
<span class="go">                    &#39;value&#39;: &#39;2019-01-12T12:00:00.000-08:00&#39;},</span>
<span class="go">             &#39;type&#39;: &#39;interval&#39;}},</span>
<span class="go">   .</span>
<span class="go">   .</span>
<span class="go">   .</span>
<span class="go">  {&#39;body&#39;: &#39;9am&#39;,</span>
<span class="go">   &#39;dim&#39;: &#39;time&#39;,</span>
<span class="go">   &#39;end&#39;: 23,</span>
<span class="go">   &#39;latent&#39;: False,</span>
<span class="go">   &#39;start&#39;: 20,</span>
<span class="go">   &#39;value&#39;: {&#39;grain&#39;: &#39;hour&#39;,</span>
<span class="go">             &#39;type&#39;: &#39;value&#39;,</span>
<span class="go">             &#39;value&#39;: &#39;2019-01-12T09:00:00.000-08:00&#39;}}],</span>
<span class="go"> 200)</span>
</pre></div>
</div>
<p>The <code class="xref py py-func docutils literal"><span class="pre">parse_numerics()</span></code> function returns a tuple where the first item is a list of dictionaries
with each one representing an extracted entity and the second item is an HTTP status code.
Each dictionary in this list represents a token span that Workbench has detected as a system entity.
Dictionaries can have overlapping spans if text could correspond to multiple system entities.</p>
<p>Significant keys and values within these inner dictionaries are shown in the table below.</p>
<table border="1" class="docutils">
<colgroup>
<col width="11%" />
<col width="42%" />
<col width="47%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Key</th>
<th class="head">Value</th>
<th class="head">Meaning or content</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>start</td>
<td>Non-negative integer</td>
<td>The start index of the entity</td>
</tr>
<tr class="row-odd"><td>end</td>
<td>Non-negative integer</td>
<td>The end index of the entity</td>
</tr>
<tr class="row-even"><td>body</td>
<td>Text</td>
<td>The text of the detected entity</td>
</tr>
<tr class="row-odd"><td>dim</td>
<td><code class="docutils literal"><span class="pre">time</span></code> , <code class="docutils literal"><span class="pre">number</span></code> , or another label</td>
<td>The type of the numeric entity</td>
</tr>
<tr class="row-even"><td>latent</td>
<td>Boolean</td>
<td>False if the entity contains all necessary
information to be an instance of that dimension,
True otherwise. E.g. ‘9AM’ would have
<code class="docutils literal"><span class="pre">latent=False</span></code> for the time dimension. But
‘9’ would have <code class="docutils literal"><span class="pre">latent=True</span></code> for the
amount-of-money dimension.</td>
</tr>
<tr class="row-odd"><td>value</td>
<td>Dictionary with ‘value’, ‘grain’, ‘type’</td>
<td>A dictionary of information about the entity.
The ‘value’ key corresponds to the resolved
value, the ‘grain’ key is the granularity of the
resolved value, and the ‘type’ is either ‘value’
or ‘interval’.</td>
</tr>
</tbody>
</table>
<p>This output is especially useful when debugging system entity behavior.</p>
</div>
<div class="section" id="when-workbench-is-unable-to-resolve-a-system-entity">
<h3>When Workbench is unable to resolve a system entity<a class="headerlink" href="#when-workbench-is-unable-to-resolve-a-system-entity" title="Permalink to this headline">¶</a></h3>
<p>Two common mistakes when working with system entities are: annotating an entity as the wrong type, and, labeling an unsupported token as an entity. In these cases, Workbench will be unable to resolve the system entity.</p>
<p><strong>Annotating a system entity as the wrong type</strong></p>
<p>Because <code class="docutils literal"><span class="pre">sys_interval</span></code> and <code class="docutils literal"><span class="pre">sys_time</span></code> are so close in meaning, developers or annotation scripts sometimes use one in place of the other.</p>
<p>In the example below, both entities should be annotated as <code class="docutils literal"><span class="pre">sys_time</span></code>, but one was mislabeled as <code class="docutils literal"><span class="pre">sys_interval</span></code>:</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>change my {6:45|sys_interval|old_time} alarm to {7 am|sys_time|new_time}
</pre></div>
</div>
<p>Workbench prints the following error during training:</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>Unable to load query: Unable to resolve system entity of type &#39;sys_interval&#39; for &#39;6:45&#39;. Entities found for the following types [&#39;sys_time&#39;]
</pre></div>
</div>
<p>The solution is to change the first entity to <code class="docutils literal"><span class="pre">{6:45|sys_time|old_time}</span></code>.</p>
<p><strong>Unsupported tokens in system entities</strong></p>
<p>Not all reasonable-sounding tokens are actually supported by a Workbench system entity.</p>
<p>In the example below, the token “daily” is annotated as a <code class="docutils literal"><span class="pre">sys_time</span></code> entity:</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>set my alarm {daily|sys_time}
</pre></div>
</div>
<p>Workbench prints the following error during training:</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>Unable to load query: Unable to resolve system entity of type &#39;sys_time&#39; for &#39;daily&#39;.
</pre></div>
</div>
<p>Possible solutions:</p>
<ol class="arabic simple">
<li>Add a custom entity that supports the token in question. For example, a <code class="docutils literal"><span class="pre">recurrence</span></code> custom entity could support tokens like “daily”, “weekly”, and so on. The correctly-annotated query would be “set my alarm {daily|recurrence}”.</li>
<li>Remove the entity label from tokens like “daily” and see if the app satisfactorily handles the queries anyway.</li>
<li>Remove all queries that contain unsupported tokens like “daily” entirely from the training data.</li>
</ol>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="lstm.html" class="btn btn-neutral float-right" title="Using LSTM for Entity Recognition" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="intent_classifier.html" class="btn btn-neutral float-left" title="Working with the Intent Classifier" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Cisco Systems

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>