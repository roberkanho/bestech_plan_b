

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Dealing with Voice Inputs &mdash; The Conversational AI Playbook 4.0.2 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'../',
              VERSION:'4.0.2',
              LANGUAGE:'None',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/custom.js"></script>
        <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@1/dist/clipboard.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Recent Changes" href="../versions/changes.html" />
    <link rel="prev" title="Working with the Dialogue Manager" href="dm.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> The Conversational AI Playbook
          

          
          </a>

          
            
            
              <div class="version">
                4.0.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/introduction_to_conversational_applications.html">Introduction to Conversational Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/approaches_for_building_conversational_applications.html">Different Approaches for Building Conversational Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/anatomy_of_a_conversational_ai_interaction.html">Anatomy of a Conversational AI Interaction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/introducing_mindmeld_workbench.html">Introducing MindMeld</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/key_concepts.html">Key Concepts</a></li>
</ul>
<p class="caption"><span class="caption-text">Step-by-Step Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/00_overview.html">Building a Conversational Interface in 10 Steps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/01_select_the_right_use_case.html">Step 1: Select the Right Use Case</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/02_script_interactions.html">Step 2: Script Your Ideal Dialogue Interactions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/03_define_the_hierarchy.html">Step 3: Define the Domain, Intent, Entity, and Role Hierarchy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/04_define_the_dialogue_handlers.html">Step 4: Define the Dialogue State Handlers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/05_create_the_knowledge_base.html">Step 5: Create the Knowledge Base</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/06_generate_representative_training_data.html">Step 6: Generate Representative Training Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/07_train_the_natural_language_processing_classifiers.html">Step 7: Train the Natural Language Processing Classifiers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/08_configure_the_language_parser.html">Step 8: Configure the Language Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/09_optimize_question_answering_performance.html">Step 9: Optimize Question Answering Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart/10_deploy_to_production.html">Step 10: Deploy Trained Models</a></li>
</ul>
<p class="caption"><span class="caption-text">Blueprint Applications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../blueprints/overview.html">MindMeld Blueprints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blueprints/food_ordering.html">Food Ordering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blueprints/video_discovery.html">Video Discovery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blueprints/home_assistant.html">Home Assistant</a></li>
</ul>
<p class="caption"><span class="caption-text">Integrations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../integrations/webex_teams.html">Webex Teams Integration</a></li>
</ul>
<p class="caption"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="about.html">About this guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="architecture.html">Platform Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="preprocessor.html">Working with the Preprocessor</a></li>
<li class="toctree-l1"><a class="reference internal" href="nlp.html">Working with the Natural Language Processor</a></li>
<li class="toctree-l1"><a class="reference internal" href="domain_classifier.html">Working with the Domain Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="intent_classifier.html">Working with the Intent Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="entity_recognizer.html">Working with the Entity Recognizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="lstm.html">Using LSTM for Entity Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="role_classifier.html">Working with the Role Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="entity_resolver.html">Working with the Entity Resolver</a></li>
<li class="toctree-l1"><a class="reference internal" href="parser.html">Working with the Language Parser</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_features.html">Working with User-Defined Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="kb.html">Working with the Knowledge Base and Question Answerer</a></li>
<li class="toctree-l1"><a class="reference internal" href="dm.html">Working with the Dialogue Manager</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Dealing with Voice Inputs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#challenges-with-speech-recognition">Challenges with speech recognition</a></li>
<li class="toctree-l2"><a class="reference internal" href="#phonetic-matching-in-entity-resolution">Phonetic matching in entity resolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="#leveraging-asr-n-best-lists">Leveraging ASR n-best lists</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Versions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../versions/changes.html">Recent Changes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../versions/history.html">Package History</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../internal/api_reference.html">API Reference</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">The Conversational AI Playbook</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Dealing with Voice Inputs</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/userguide/voice.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="dealing-with-voice-inputs">
<h1>Dealing with Voice Inputs<a class="headerlink" href="#dealing-with-voice-inputs" title="Permalink to this headline">¶</a></h1>
<p>MindMeld Workbench provides all the functionality required to build a text-based natural language
chat interface. However, you might also want to support voice interactions with your application
or build for a platform where voice is the only input modality. For such applications, you can
leverage a third party
<a class="reference external" href="https://en.wikipedia.org/wiki/Speech_recognition">Automatic Speech Recognition</a> (ASR) system to
convert the input speech into text. There are multiple third party speech recognition systems available such as those from <a class="reference external" href="https://cloud.google.com/speech-to-text/">Google</a>,
<a class="reference external" href="https://soundhound.com/houndify">Hound</a>,
<a class="reference external" href="https://www.nuance.com/mobile/speech-recognition-solutions.html">Nuance</a>, and
<a class="reference external" href="https://azure.microsoft.com/en-us/services/cognitive-services/speech/?v=18.05">Microsoft</a>.
The converted text transcript can then be processed by your Workbench application to return an
appropriate text response. Finally, you can send this text response to a third party
<a class="reference external" href="https://en.wikipedia.org/wiki/Speech_synthesis">Text to Speech</a> (TTS) system to synthesize an
audio response that can be “spoken” back to the user.  Similar to speech recognition, there are multiple third party TTS systems available for use, including but not limited to the services
provided by <a class="reference external" href="https://aws.amazon.com/polly/">Amazon</a>,
<a class="reference external" href="https://cloud.google.com/text-to-speech/">Google</a> and
<a class="reference external" href="https://azure.microsoft.com/en-us/services/cognitive-services/text-to-speech/">Microsoft</a>.</p>
<div class="section" id="challenges-with-speech-recognition">
<h2>Challenges with speech recognition<a class="headerlink" href="#challenges-with-speech-recognition" title="Permalink to this headline">¶</a></h2>
<p>It’s important to note that speech recognition is not perfect, especially for domain-specific
vocabulary and proper nouns. ASR errors may cause the text input to your Workbench application to
be something different than what the user intended, which can lead to an unexpected response that
makes the app appear unintelligent. The part of the pipeline that is most susceptible to a drop in
accuracy due to ASR errors is the entity resolution step. This is because domain and intent
classification rely more on sentence structure and context words which generic speech recognition
systems tend to get right. However, domain-specific entities, which are generally less common
words or proper nouns, are likely to be mistranscribed by generic third party speech recognition
systems. These terms are often transcribed to tokens that are phonetically similar but textually
significantly different from what was said.</p>
<p>One way to overcome this problem is by building your own custom domain ASR system if you have
enough audio data available for training. This is a larger task which we won’t describe here, but
there are a variety of open source models available as a starting point including
<a class="reference external" href="https://github.com/mozilla/DeepSpeech">Mozilla Deep Speech</a>,
<a class="reference external" href="https://cmusphinx.github.io/">CMUSphinx</a>, and <a class="reference external" href="https://github.com/kaldi-asr/kaldi">Kaldi</a>.
However, given the cost and effort associated with building ASR models from scratch, the most
common scenario is to use an out-of-the-box ASR. In the following sections, we will describe a
couple of techniques you can leverage in Workbench to maintain a high entity resolution accuracy
despite speech recognition errors.</p>
</div>
<div class="section" id="phonetic-matching-in-entity-resolution">
<h2>Phonetic matching in entity resolution<a class="headerlink" href="#phonetic-matching-in-entity-resolution" title="Permalink to this headline">¶</a></h2>
<p>The Workbench Entity Resolver is optimized for typed inputs by default. It can handle text
variations like typos and misspellings, and it leverages synonym lists to resolve terms that are
semantically similar. In addition to these variations, for voice inputs, there is a large category
of entity variations that are phonetically similar but textually different from the canonical form.
For example, the entity “Arnold Schwarzenegger” may be mistranscribed to “our old shorts hanger”.
These two terms sound similar but have little character overlap. To resolve these types of
mistranscriptions, you can enable phonetic matching in the entity resolver by specifying the
<code class="docutils literal"><span class="pre">phonetic_match_types</span></code> parameter in your entity resolution config.</p>
<p>The value corresponding to the <code class="docutils literal"><span class="pre">phonetic_match_types</span></code> key is a list of phonetic encoding
techniques to use for entity matching. There are a few common techniques that are used to generate
the phonetic representation of text, of which, one of the most optimized and efficient is the
<a class="reference external" href="https://en.wikipedia.org/wiki/Metaphone#Double_Metaphone">Double Metaphone</a> algorithm. Double
Metaphone is based on a series of rules optimized for indexing the phonetic representations of
names of various origins. Currently, <code class="docutils literal"><span class="pre">double_metaphone</span></code> is the only supported value for
<code class="docutils literal"><span class="pre">phonetic_match_types</span></code>. It can be specified in your application configuration file (config.py)
as follows.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">ENTITY_RESOLVER_CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;model_type&#39;</span><span class="p">:</span> <span class="s1">&#39;text_relevance&#39;</span><span class="p">,</span>
    <span class="s1">&#39;phonetic_match_types&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;double_metaphone&#39;</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If the <code class="docutils literal"><span class="pre">phonetic_match_types</span></code> key is specified in the entity resolution config, the resolver
tries to phonetically match extracted entity spans against your entity index in addition to using
the default text-based matching techniques. This can improve the relevance of the ranked results
returned by the entity resolver as illustrated in the following example.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li>In order to utilize phonetic matching, you must install the phonetic analysis plugin for
Elasticsearch and restart the Elasticsearch service. Refer to <a class="reference external" href="https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-phonetic.html">Elasticsearch documentation</a>
for instructions. You may also have to delete the default Elasticsearch template by running
this command in your shell: <code class="docutils literal"><span class="pre">curl</span> <span class="pre">-X</span> <span class="pre">DELETE</span> <span class="pre">&quot;localhost:9200/_template/default&quot;</span></code>.</li>
<li>After first introducing <code class="docutils literal"><span class="pre">phonetic_match_types</span></code> to the entity resolver config, restart your
Python shell and rebuild the entity resolver index from scratch by running a clean fit
(<code class="docutils literal"><span class="pre">er.fit(clean=True)</span></code>). See the <a class="reference internal" href="entity_resolver.html"><span class="doc">Entity Resolver</span></a> page for
additional details.</li>
</ul>
</div>
<p>Consider a setting where the user says “I want to eat some Pad Thai and Yellow Curry”, and the ASR
transcribes it as “i want to beat some pad thai and mellow Kerrie”. Let us see the resolved values
for the span “mellow Kerrie” without using phonetic matching.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mmworkbench</span> <span class="kn">import</span> <span class="n">configure_logs</span><span class="p">;</span> <span class="n">configure_logs</span><span class="p">()</span>
<span class="kn">from</span> <span class="nn">mmworkbench.components.nlp</span> <span class="kn">import</span> <span class="n">NaturalLanguageProcessor</span>
<span class="kn">from</span> <span class="nn">mmworkbench.core</span> <span class="kn">import</span> <span class="n">Entity</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">NaturalLanguageProcessor</span><span class="p">(</span><span class="n">app_path</span><span class="o">=</span><span class="s1">&#39;food_ordering&#39;</span><span class="p">)</span>
<span class="n">nlp</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
<span class="n">nlp</span><span class="o">.</span><span class="n">dump</span><span class="p">()</span>
<span class="hll"><span class="n">er</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">domains</span><span class="p">[</span><span class="s1">&#39;ordering&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">intents</span><span class="p">[</span><span class="s1">&#39;build_order&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">entities</span><span class="p">[</span><span class="s1">&#39;dish&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">entity_resolver</span>
</span><span class="hll"><span class="n">er</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Entity</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s1">&#39;mellow Kerrie&#39;</span><span class="p">,</span> <span class="n">entity_type</span><span class="o">=</span><span class="s1">&#39;dish&#39;</span><span class="p">))</span>
</span></pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">{&#39;cname&#39;: &#39;60. Crispy Fried Portobello Mushroom&#39;,</span>
<span class="go"> &#39;id&#39;: &#39;B01CLQSAVG&#39;,</span>
<span class="go"> &#39;score&#39;: 21.071535,</span>
<span class="go"> &#39;top_synonym&#39;: &#39;60. Crispy Fried Portobello Mushroom&#39;},</span>
<span class="go">{&#39;cname&#39;: &#39;61. Crispy Fried Portobello Mushroom&#39;,</span>
<span class="go"> &#39;id&#39;: &#39;B01CLQS9ZI&#39;,</span>
<span class="go"> &#39;score&#39;: 19.876467,</span>
<span class="go"> &#39;top_synonym&#39;: &#39;61. Crispy Fried Portobello Mushroom&#39;},</span>
<span class="go">{&#39;cname&#39;: &#39;Fried Mozzarella&#39;,</span>
<span class="go"> &#39;id&#39;: &#39;B01ENMPJPG&#39;,</span>
<span class="go"> &#39;score&#39;: 19.337563,</span>
<span class="go"> &#39;top_synonym&#39;: &#39;Fried Mozzarella&#39;},</span>
<span class="go">{&#39;cname&#39;: &#39;Fried Mozzarella Sticks&#39;,</span>
<span class="go"> &#39;id&#39;: &#39;B01N18BT3F&#39;,</span>
<span class="go"> &#39;score&#39;: 18.817226,</span>
<span class="go"> &#39;top_synonym&#39;: &#39;fried mozzarella&#39;},</span>
<span class="go">{&#39;cname&#39;: &#39;Twin Scallops Fried Rice&#39;,</span>
<span class="go"> &#39;id&#39;: &#39;B01CIKMRB4&#39;,</span>
<span class="go"> &#39;score&#39;: 18.401768,</span>
<span class="go"> &#39;top_synonym&#39;: &#39;scallops fried rice&#39;},</span>
<span class="go">{&#39;cname&#39;: &#39;36. Spicy Prawn &amp; Crispy Fried Portobello Mushroom&#39;,</span>
<span class="go"> &#39;id&#39;: &#39;B01CLQSEYE&#39;,</span>
<span class="go"> &#39;score&#39;: 18.10899,</span>
<span class="go"> &#39;top_synonym&#39;: &#39;36. Spicy Prawn &amp; Crispy Fried Portobello Mushroom&#39;},</span>
<span class="go">{&#39;cname&#39;: &#39;Hamachi (Yellow Tail)&#39;,</span>
<span class="go"> &#39;id&#39;: &#39;B01MRRJDRC&#39;,</span>
<span class="go"> &#39;score&#39;: 15.289129,</span>
<span class="go"> &#39;top_synonym&#39;: &#39;Hamachi (Yellow Tail)&#39;},</span>
<span class="go">{&#39;cname&#39;: &#39;Yellow Sea&#39;,</span>
<span class="go"> &#39;id&#39;: &#39;B01CPOE9BE&#39;,</span>
<span class="go"> &#39;score&#39;: 14.5856,</span>
<span class="go"> &#39;top_synonym&#39;: &#39;Yellow Sea&#39;},</span>
<span class="hll"><span class="go">{&#39;cname&#39;: &#39;Yellow Curry&#39;,</span>
</span><span class="hll"><span class="go"> &#39;id&#39;: &#39;B01CPOEBC6&#39;,</span>
</span><span class="hll"><span class="go"> &#39;score&#39;: 14.556676,</span>
</span><span class="hll"><span class="go"> &#39;top_synonym&#39;: &#39;Yellow Curry&#39;},</span>
</span><span class="go">{&#39;cname&#39;: &#39;Tuna Melt Sandwich&#39;,</span>
<span class="go"> &#39;id&#39;: &#39;B01CH0SPK2&#39;,</span>
<span class="go"> &#39;score&#39;: 14.51431,</span>
<span class="go"> &#39;top_synonym&#39;: &#39;tuna melt grinder with fries&#39;}]</span>
</pre></div>
</div>
<p>In the absence of phonetic information, the resolution results do not resemble what the user
originally said. You can see that the top result has character overlaps with the mistranscription
(“ello” , “rie” , etc.), but it is clearly not what the user intended (“Yellow Curry”). There is
just enough character overlap to rank “Yellow Curry” in the ninth spot but the remaining results
are unrelated (“Fried Portobello”, “Fried Mozzarella”, etc.).</p>
<p>Next, let us see the resolved values for “mellow Kerrie” with phonetic matching enabled in the
config.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># After updating app config and restarting the Python shell</span>
<span class="kn">from</span> <span class="nn">mmworkbench</span> <span class="kn">import</span> <span class="n">configure_logs</span><span class="p">;</span> <span class="n">configure_logs</span><span class="p">()</span>
<span class="kn">from</span> <span class="nn">mmworkbench.components.nlp</span> <span class="kn">import</span> <span class="n">NaturalLanguageProcessor</span>
<span class="kn">from</span> <span class="nn">mmworkbench.core</span> <span class="kn">import</span> <span class="n">Entity</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">NaturalLanguageProcessor</span><span class="p">(</span><span class="n">app_path</span><span class="o">=</span><span class="s1">&#39;food_ordering&#39;</span><span class="p">)</span>
<span class="n">nlp</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
<span class="hll"><span class="n">er</span> <span class="o">=</span> <span class="n">nlp</span><span class="o">.</span><span class="n">domains</span><span class="p">[</span><span class="s1">&#39;ordering&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">intents</span><span class="p">[</span><span class="s1">&#39;build_order&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">entities</span><span class="p">[</span><span class="s1">&#39;dish&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">entity_resolver</span>
</span><span class="hll"><span class="n">er</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">clean</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="hll"><span class="n">er</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Entity</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s1">&#39;mellow Kerrie&#39;</span><span class="p">,</span> <span class="n">entity_type</span><span class="o">=</span><span class="s1">&#39;dish&#39;</span><span class="p">))</span>
</span></pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="hll"><span class="go">{&#39;cname&#39;: &#39;Yellow Curry&#39;,</span>
</span><span class="hll"><span class="go">  &#39;id&#39;: &#39;B01CPOEBC6&#39;,</span>
</span><span class="hll"><span class="go">  &#39;score&#39;: 25.13264,</span>
</span><span class="hll"><span class="go">  &#39;top_synonym&#39;: &#39;Yellow Curry&#39;},</span>
</span><span class="go"> {&#39;cname&#39;: &#39;60. Crispy Fried Portobello Mushroom&#39;,</span>
<span class="go">  &#39;id&#39;: &#39;B01CLQSAVG&#39;,</span>
<span class="go">  &#39;score&#39;: 21.071535,</span>
<span class="go">  &#39;top_synonym&#39;: &#39;60. Crispy Fried Portobello Mushroom&#39;},</span>
<span class="go"> {&#39;cname&#39;: &#39;61. Crispy Fried Portobello Mushroom&#39;,</span>
<span class="go">  &#39;id&#39;: &#39;B01CLQS9ZI&#39;,</span>
<span class="go">  &#39;score&#39;: 19.876467,</span>
<span class="go">  &#39;top_synonym&#39;: &#39;61. Crispy Fried Portobello Mushroom&#39;},</span>
<span class="hll"><span class="go"> {&#39;cname&#39;: &#39;79. Kao Pad Goong Pong-Ga-Ree Fried Rice&#39;,</span>
</span><span class="hll"><span class="go">  &#39;id&#39;: &#39;B01LY4ZA0M&#39;,</span>
</span><span class="hll"><span class="go">  &#39;score&#39;: 19.338999,</span>
</span><span class="hll"><span class="go">  &#39;top_synonym&#39;: &#39;yellow curry and shrimp fried rice&#39;},</span>
</span><span class="go"> {&#39;cname&#39;: &#39;Fried Mozzarella&#39;,</span>
<span class="go">  &#39;id&#39;: &#39;B01ENMPJPG&#39;,</span>
<span class="go">  &#39;score&#39;: 19.337563,</span>
<span class="go">  &#39;top_synonym&#39;: &#39;Fried Mozzarella&#39;},</span>
<span class="go"> {&#39;cname&#39;: &#39;Fried Mozzarella Sticks&#39;,</span>
<span class="go">  &#39;id&#39;: &#39;B01N18BT3F&#39;,</span>
<span class="go">  &#39;score&#39;: 18.817226,</span>
<span class="go">  &#39;top_synonym&#39;: &#39;fried mozzarella&#39;},</span>
<span class="go"> {&#39;cname&#39;: &#39;Twin Scallops Fried Rice&#39;,</span>
<span class="go">  &#39;id&#39;: &#39;B01CIKMRB4&#39;,</span>
<span class="go">  &#39;score&#39;: 18.401768,</span>
<span class="go">  &#39;top_synonym&#39;: &#39;scallops fried rice&#39;},</span>
<span class="go"> {&#39;cname&#39;: &#39;36. Spicy Prawn &amp; Crispy Fried Portobello Mushroom&#39;,</span>
<span class="go">  &#39;id&#39;: &#39;B01CLQSEYE&#39;,</span>
<span class="go">  &#39;score&#39;: 18.10899,</span>
<span class="go">  &#39;top_synonym&#39;: &#39;36. Spicy Prawn &amp; Crispy Fried Portobello Mushroom&#39;},</span>
<span class="hll"><span class="go"> {&#39;cname&#39;: &#39;Panang Curry (Over Rice)&#39;,</span>
</span><span class="hll"><span class="go">  &#39;id&#39;: &#39;B01DV7324O&#39;,</span>
</span><span class="hll"><span class="go">  &#39;score&#39;: 17.12096,</span>
</span><span class="hll"><span class="go">  &#39;top_synonym&#39;: &#39;Creamy yellow tofu curry&#39;},</span>
</span><span class="hll"><span class="go"> {&#39;cname&#39;: &#39;Roti with Curry&#39;,</span>
</span><span class="hll"><span class="go">  &#39;id&#39;: &#39;B01LX5THED&#39;,</span>
</span><span class="hll"><span class="go">  &#39;score&#39;: 15.802841,</span>
</span><span class="hll"><span class="go">  &#39;top_synonym&#39;: &#39;roti with yellow curry&#39;}]</span>
</span></pre></div>
</div>
<p>These results look more reasonable. The top result exactly matches the user’s intended dish,
“Yellow Curry” due to its high phonetic similarity to the extracted entity “mellow Kerrie”. Many
other results have also been ranked higher due to phonetic matches against the canonical name or
the synonym list.</p>
</div>
<div class="section" id="leveraging-asr-n-best-lists">
<span id="nbest-lists"></span><h2>Leveraging ASR n-best lists<a class="headerlink" href="#leveraging-asr-n-best-lists" title="Permalink to this headline">¶</a></h2>
<p>Almost all out-of-the-box third party ASR APIs return a ranked list of multiple possible
transcripts, also called an <em>n-best list</em>. For example, if the user said “Look for movies
directed by Tarantino”, the n-best list of recognition hypotheses may look like the following:</p>
<div class="highlight-text"><div class="highlight"><pre><span></span>[&#39;look for movies directed by Terren Tina&#39;,
 &#39;look for movies directed by Darren Tina&#39;,
 &#39;look for movies directed by Tarantino&#39;,
 &#39;look for movies directed by tear and tea no&#39;,
 &#39;look for movies directed by Terren teen&#39;]
</pre></div>
</div>
<p>This list of transcripts represents the top guesses by the speech recognition language model given
the phonemes in the audio file. For uncommon terms, the correct transcription may be in a lower
transcript or not in the list at all. However, using the phonetic information from the n-best list
and app-specific context, we can resolve entities to their intended values with a high accuracy.</p>
<p>Workbench provides the option to pass in the n-best list of ASR transcripts for extracting
multiple candidate entities to improve entity resolution.</p>
<p>To leverage this functionality add an <code class="docutils literal"><span class="pre">NLP_CONFIG</span></code> dictionary to your application configuration
file (<code class="docutils literal"><span class="pre">config.py</span></code>) as follows.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">NLP_CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;resolve_entities_using_nbest_transcripts&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;video_content.*&#39;</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Specify the domains and intents of interest by including them as an element in the list value
corresponding to the <code class="docutils literal"><span class="pre">resolve_entities_using_nbest_transcripts</span></code> key. The format is
<code class="docutils literal"><span class="pre">&lt;domain&gt;.&lt;intent&gt;</span></code>, and an asterisk (‘*’) wildcard denotes <em>all</em> intents within the specified
domain.</p>
<p>Workbench will limit running the n-best enhanced entity resolution to the domains and intents you
have specified. This is an optimization to avoid unnecessary processing of a factor of
<cite>n</cite> for queries without entities of interest. While the code is parallelized for minimal latency
increase, there will be an increase in memory usage from the domains and intents for which n-best
entity processing is run. You can control the parallel processing behavior using the
<a class="reference internal" href="getting_started.html#parallel-processing"><span class="std std-ref">MM_SUBPROCESS_COUNT</span></a> enviroment variable.</p>
<p>Also make sure that you have phonetic matching enabled for the entity resolver in your app config.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">ENTITY_RESOLVER_CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;model_type&#39;</span><span class="p">:</span> <span class="s1">&#39;text_relevance&#39;</span><span class="p">,</span>
    <span class="s1">&#39;phonetic_match_types&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;double_metaphone&#39;</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Once the app config is updated, you can pass in a list or tuple of strings to <code class="xref py py-meth docutils literal"><span class="pre">nlp.process()</span></code>
instead of a single string. If the top transcript gets classified as one of the domains or intents
specified for n-best enhanced entity resolution, information from the entire n-best list will be
used for resolving the entity.</p>
<p>Let us see the results of n-best enhanced entity resolution for the above example where the user said “Look for movies directed by Tarantino”. Note that we pass the entire the n-best list of ASR transcripts to <code class="xref py py-meth docutils literal"><span class="pre">nlp.process()</span></code>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">nlp</span><span class="o">.</span><span class="n">process</span><span class="p">([</span><span class="s1">&#39;look for movies directed by Terren Tina&#39;</span><span class="p">,</span>
             <span class="s1">&#39;look for movies directed by Darren Tina&#39;</span><span class="p">,</span>
             <span class="s1">&#39;look for movies directed by Tarantino&#39;</span><span class="p">,</span>
             <span class="s1">&#39;look for movies directed by tear and tea no&#39;</span><span class="p">,</span>
             <span class="s1">&#39;look for movies directed by Terren teen&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go"> {</span>
<span class="go">  &#39;domain&#39;: &#39;video_content&#39;,</span>
<span class="go">  &#39;entities&#39;: [{&#39;role&#39;: None,</span>
<span class="go">    &#39;span&#39;: {&#39;end&#39;: 39, &#39;start&#39;: 28},</span>
<span class="go">    &#39;text&#39;: &#39;Terren Tina&#39;,</span>
<span class="go">    &#39;type&#39;: &#39;director&#39;,</span>
<span class="hll"><span class="go">    &#39;value&#39;: [{&#39;cname&#39;: &#39;Quentin Tarantino&#39;,</span>
</span><span class="hll"><span class="go">      &#39;id&#39;: &#39;B01CPOEKPY&#39;,</span>
</span><span class="hll"><span class="go">      &#39;score&#39;: 176.69968,</span>
</span><span class="hll"><span class="go">      &#39;top_synonym&#39;: &#39;Tarantino&#39;},</span>
</span><span class="go">     {&#39;cname&#39;: &#39;Tina Mabry&#39;,</span>
<span class="go">      &#39;id&#39;: &#39;B01G67O8GM&#39;,</span>
<span class="go">      &#39;score&#39;: 128.46222,</span>
<span class="go">      &#39;top_synonym&#39;: &#39;Tina&#39;},</span>
<span class="go">     {&#39;cname&#39;: &#39;51. Darren Aronofsky&#39;,</span>
<span class="go">      &#39;id&#39;: &#39;B01LXTA7WA&#39;,</span>
<span class="go">      &#39;score&#39;: 42.02176,</span>
<span class="go">      &#39;top_synonym&#39;: &#39;Darren&#39;},</span>
<span class="go">      ...</span>
<span class="go">      ]}],</span>
<span class="go">  &#39;intent&#39;: &#39;browse&#39;,</span>
<span class="hll"><span class="go">  &#39;nbest_aligned_entities&#39;: [</span>
</span><span class="go">     [{&#39;text&#39;: &#39;Terren Tina&#39;, &#39;type&#39;: director},</span>
<span class="go">      {&#39;text&#39;: &#39;Darren Tina&#39;, &#39;type&#39;: director},</span>
<span class="go">      {&#39;text&#39;: &#39;Tarantino&#39;, &#39;type&#39;: director},</span>
<span class="go">      {&#39;text&#39;: &#39;tear and tea non&#39;, &#39;type&#39;: director},</span>
<span class="go">      {&#39;text&#39;: &#39;Terren teen&#39;, &#39;type&#39;: director}</span>
<span class="go">     ]</span>
<span class="go">   ],</span>
<span class="hll"><span class="go">  &#39;nbest_transcripts_entities&#39;: [</span>
</span><span class="go">     [{&#39;role&#39;: None,</span>
<span class="go">       &#39;span&#39;: {&#39;end&#39;: 39, &#39;start&#39;: 28},</span>
<span class="go">       &#39;text&#39;: &#39;Terren Tina&#39;,</span>
<span class="go">       &#39;type&#39;: &#39;director&#39;}],</span>
<span class="go">     [{&#39;role&#39;: None,</span>
<span class="go">       &#39;span&#39;: {&#39;end&#39;: 39, &#39;start&#39;: 28},</span>
<span class="go">       &#39;text&#39;: &#39;Darren Tina&#39;,</span>
<span class="go">       &#39;type&#39;: &#39;director&#39;}],</span>
<span class="go">     [{&#39;role&#39;: None,</span>
<span class="go">       &#39;span&#39;: {&#39;end&#39;: 37, &#39;start&#39;: 28},</span>
<span class="go">       &#39;text&#39;: &#39;Tarantino&#39;,</span>
<span class="go">       &#39;type&#39;: &#39;director&#39;}],</span>
<span class="go">     [{&#39;role&#39;: None,</span>
<span class="go">       &#39;span&#39;: {&#39;end&#39;: 43, &#39;start&#39;: 28},</span>
<span class="go">       &#39;text&#39;: &#39;tear and tea non&#39;,</span>
<span class="go">       &#39;type&#39;: &#39;director&#39;}],</span>
<span class="go">     [{&#39;role&#39;: None,</span>
<span class="go">       &#39;span&#39;: {&#39;end&#39;: 39, &#39;start&#39;: 28},</span>
<span class="go">       &#39;text&#39;: &#39;Terren teen&#39;,</span>
<span class="go">       &#39;type&#39;: &#39;director&#39;}]],</span>
<span class="hll"><span class="go">  &#39;nbest_transcripts_text&#39;: [</span>
</span><span class="go">     &#39;look for movies directed by Terren Tina&#39;,</span>
<span class="go">     &#39;look for movies directed by Darren Tina&#39;,</span>
<span class="go">     &#39;look for movies directed by Tarantino&#39;,</span>
<span class="go">     &#39;look for movies directed by tear and tea no&#39;,</span>
<span class="go">     &#39;look for movies directed by Terren teen&#39;],</span>
<span class="go">  &#39;text&#39;: &#39;look for movies directed by Terren Tina&#39;}</span>
</pre></div>
</div>
<p>You can see that the query was classified as the <code class="docutils literal"><span class="pre">video_content</span></code> domain and the <code class="docutils literal"><span class="pre">browse</span></code>
intent. Since all intents in the <code class="docutils literal"><span class="pre">video_content</span></code> domain were specified in the
<code class="docutils literal"><span class="pre">NLP_CONFIG</span></code> above, Workbench ran n-best entity processing for this query. This involves running
entity recognition on all the n-best transcripts and using information from the all of the
extracted entities for entity resolution.</p>
<p>In this example, the n-best transcripts had multiple examples of phonetic matches to
“Tarantino”, and one of the hypotheses even had the exact correct transcription of “Tarantino”. By
using the entities extracted from all of these transcripts, the entity resolver was able to
correctly get the top entity as “Quentin Tarantino”. Without using n-best entity resolution, the
phonetic matching against just the top transcript “Terren Tina” may not be enough to differentiate
between similar names like “Darren Lima”. The n-best transcripts often provide additional
phonetic information to improve the accuracy of resolving to the intended entity.</p>
<p>While the built-in entity resolver that leverages phonetic information and n-best transcripts is a
great starting point for dealing with ASR errors, in many cases you can further improve accuracy
by leveraging application-specific context. To enable this, the NLP response includes a few
additional fields that you can you use in the dialogue manager as you see fit:</p>
<table border="1" class="docutils">
<colgroup>
<col width="38%" />
<col width="63%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Key</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="xref py py-data docutils literal"><span class="pre">nbest_transcripts_text</span></code></td>
<td>The input list of n-best transcripts.</td>
</tr>
<tr class="row-odd"><td><code class="xref py py-data docutils literal"><span class="pre">nbest_transcripts_entities</span></code></td>
<td><p class="first">A list of lists, one for each input transcript. Each
sublist contains a list of extracted entities for that
transcript.</p>
<p class="last">For example, “Terren Tina” is the extracted <code class="docutils literal"><span class="pre">director</span></code>
entity from the first transcript “look for movies
directed by Terren Tina”, “Darren Tina” is the extracted
<code class="docutils literal"><span class="pre">director</span></code> entity from the second transcript “look for
movies directed by Darren Tina”, and so on.</p>
</td>
</tr>
<tr class="row-even"><td><code class="xref py py-data docutils literal"><span class="pre">nbest_aligned_entities</span></code></td>
<td><p class="first">A list of lists, one for each detected entity in the input.
Each sublist contains the text spans extracted across all
the n-best transcripts for that particular entity.</p>
<p class="last">This is useful for queries with multiple entities like
“Order pad thai and spring rolls please” where both
“pad thai” and “spring rolls” are <code class="docutils literal"><span class="pre">dish</span></code> entities. In
that case, the first entry would be a list of all text
spans for the entity “pad thai” extracted across all the
n-best transcripts and the second entry would similarly be
a list of all the text spans for “spring rolls”.</p>
</td>
</tr>
</tbody>
</table>
<p>For example, you can build an app-specific entity resolver that is called from the dialogue
manager which uses all the n-best entity spans along with phonetic matching to resolve to the
correct term. To derive phonetic representations for your extracted entities, you can leverage the
<a class="reference external" href="https://en.wikipedia.org/wiki/Metaphone#Double_Metaphone">double metaphone</a> algorithm (used by
the Workbench entity resolver) or a more advanced machine-learned model like
<a class="reference external" href="https://github.com/cmusphinx/g2p-seq2seq">grapheme to phoneme</a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The domain and intent classification models solely use the top transcript to make a prediction.
The n-best transcripts are only leveraged for entity processing since those are the parts of
the NLP pipeline most susceptible to errors due to ASR mistranscriptions.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">While using phonetic matching and n-best transcripts will improve accuracy for entity
resolution on voice inputs, these approaches are not perfect. They heavily depend on the
quality of the ASR transcripts which varies with the vendor used, the background noise of the
environment, the quality of the recording device, etc. You may want to additionally include
some application-specific post processing to verify that the resolved entities are reasonable
for your use case.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../versions/changes.html" class="btn btn-neutral float-right" title="Recent Changes" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="dm.html" class="btn btn-neutral float-left" title="Working with the Dialogue Manager" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Cisco Systems

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>